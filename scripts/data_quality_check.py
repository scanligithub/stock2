# scripts/data_quality_check.py
import pandas as pd
import numpy as np
import os
import json
import datetime
import duckdb

ENGINE_DIR = "final_output/engine"
REPORT_DIR = "final_output/report"
os.makedirs(REPORT_DIR, exist_ok=True)

# ================= æ•°æ®å­—å…¸ =================
STOCK_FIELD_DESC = {
    # ç´¢å¼•
    "date": "äº¤æ˜“æ—¥æœŸ", "code": "è‚¡ç¥¨ä»£ç ",
    
    # åŸºç¡€è¡Œæƒ…
    "open": "å¼€ç›˜ä»·", "high": "æœ€é«˜ä»·", "low": "æœ€ä½ä»·", "close": "æ”¶ç›˜ä»·",
    "volume": "æˆäº¤é‡", "amount": "æˆäº¤é¢", "turn": "æ¢æ‰‹ç‡", "pctChg": "æ¶¨è·Œå¹…",
    
    # å› å­ä¸åŸºæœ¬é¢
    "peTTM": "æ»šåŠ¨å¸‚ç›ˆç‡", "pbMRQ": "å¸‚å‡€ç‡", 
    "adjustFactor": "åå¤æƒå› å­", "mkt_cap": "æµé€šå¸‚å€¼",
    
    # èµ„é‡‘æµ
    "net_flow_amount": "å‡€æµå…¥é¢ (å…¨å•)", "main_net_flow": "ä¸»åŠ›å‡€æµå…¥ (è¶…å¤§+å¤§å•)",
    "super_large_net_flow": "è¶…å¤§å•å‡€æµå…¥", "large_net_flow": "å¤§å•å‡€æµå…¥",
    "medium_small_net_flow": "ä¸­å°å•å‡€æµå…¥",
    
    # å‡çº¿
    "ma5": "5æ—¥å‡çº¿", "ma10": "10æ—¥å‡çº¿", "ma20": "20æ—¥å‡çº¿", 
    "ma60": "60æ—¥å‡çº¿", "ma120": "åŠå¹´çº¿", "ma250": "å¹´çº¿",
    
    # å‡é‡
    "vol_ma5": "5æ—¥å‡é‡", "vol_ma10": "10æ—¥å‡é‡", 
    "vol_ma20": "20æ—¥å‡é‡", "vol_ma30": "30æ—¥å‡é‡",
    
    # æŠ€æœ¯æŒ‡æ ‡
    "dif": "MACD-DIF", "dea": "MACD-DEA", "macd": "MACD-æŸ±",
    "k": "KDJ-K", "d": "KDJ-D", "j": "KDJ-J",
    "rsi6": "RSI-6", "rsi12": "RSI-12", "rsi24": "RSI-24",
    "boll_up": "å¸ƒæ—ä¸Šè½¨", "boll_lb": "å¸ƒæ—ä¸‹è½¨",
    "cci": "CCI", "atr": "ATR"
}

SECTOR_FIELD_DESC = {
    "date": "äº¤æ˜“æ—¥æœŸ", "code": "æ¿å—ä»£ç ", "name": "æ¿å—åç§°",
    "type": "ç±»å‹", "close": "æ”¶ç›˜ç‚¹ä½", "pctChg": "æ¶¨è·Œå¹…",
    "volume": "æˆäº¤é‡",
    
    # æ–°å¢æ¿å—èµ„é‡‘æµå­—æ®µ
    "net_flow_amount": "æ¿å—å‡€æµå…¥(èšåˆ)", 
    "main_net_flow": "æ¿å—ä¸»åŠ›å‡€æµå…¥(èšåˆ)"
}

def get_schema_info(df, desc_map):
    schema = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        if 'float' in dtype: dtype = 'float'
        elif 'int' in dtype: dtype = 'int'
        elif 'object' in dtype: dtype = 'string'
        schema.append({
            "name": col,
            "type": dtype,
            "desc": desc_map.get(col, "è‡ªå®šä¹‰å­—æ®µ")
        })
    return schema

def format_money(val):
    if pd.isna(val): return "N/A"
    abs_val = abs(val)
    if abs_val >= 10**8: return f"{val/10**8:.2f} äº¿"
    elif abs_val >= 10**4: return f"{val/10**4:.2f} ä¸‡"
    else: return f"{val:.2f}"

def check_stock_data():
    # æ£€æŸ¥ stock_daily ç›®å½•
    dir_path = f"{ENGINE_DIR}/stock_daily"
    print(f"ğŸ” æ£€æŸ¥ä¸ªè‚¡æ•°æ®ç›®å½•: {dir_path}/*.parquet ...")
    
    if not os.path.exists(dir_path):
        return {"status": "Error", "message": "Directory not found"}

    try:
        con = duckdb.connect()
        # ç»Ÿè®¡æ€»è¡Œæ•°å’Œæ—¥æœŸ
        base_info = con.execute(f"""
            SELECT COUNT(*), MIN(date), MAX(date), COUNT(DISTINCT code)
            FROM read_parquet('{dir_path}/*.parquet')
        """).fetchone()
        total_rows, min_date, max_date, unique_stocks = base_info
        
        # èµ„é‡‘æµç»Ÿè®¡
        ff_info = con.execute(f"""
            SELECT 
                COUNT(*) FILTER (WHERE net_flow_amount IS NULL OR net_flow_amount = 0),
                MIN(date) FILTER (WHERE net_flow_amount != 0 AND net_flow_amount IS NOT NULL),
                COUNT(*) FILTER (WHERE net_flow_amount > 0),
                COUNT(*) FILTER (WHERE net_flow_amount < 0),
                MAX(net_flow_amount)
            FROM read_parquet('{dir_path}/*.parquet')
        """).fetchone()
        anomaly_count, ff_start, pos_days, neg_days, max_in = ff_info
        
        # è¯»å– Schema (é‡‡æ ·)
        df_sample = con.execute(f"SELECT * FROM read_parquet('{dir_path}/*.parquet') LIMIT 1").fetchdf()
        
        con.close()

        score = max(0, 100 - int((anomaly_count / total_rows) * 100)) if total_rows > 0 else 0
        
        return {
            "status": "Success",
            "global_score": score,
            "total_rows": int(total_rows),
            "stock_count": int(unique_stocks),
            "date_range": f"{min_date} ~ {max_date}",
            "fund_flow": {
                "start_date": str(ff_start),
                "anomaly_count": int(anomaly_count),
                "valid_count": int(total_rows - anomaly_count),
                "details": {
                    "pos_days": int(pos_days), "neg_days": int(neg_days), 
                    "max_in": float(max_in) if max_in else 0
                }
            },
            "schema": get_schema_info(df_sample, STOCK_FIELD_DESC)
        }
    except Exception as e:
        return {"status": "Error", "message": str(e)}

def check_sector_data():
    file_path = f"{ENGINE_DIR}/sector_full.parquet"
    if not os.path.exists(file_path): return {"status": "Error", "message": "File not found"}
    
    df = pd.read_parquet(file_path)
    if len(df) == 0: return {"status": "Error", "message": "Empty"}

    # æ£€æŸ¥æ¿å—èµ„é‡‘æµæ˜¯å¦æˆåŠŸèšåˆ (é0å€¼å æ¯”)
    ff_valid = (df['net_flow_amount'] != 0).sum() if 'net_flow_amount' in df.columns else 0

    return {
        "status": "Success",
        "total_rows": int(len(df)),
        "sector_count": int(df['code'].nunique()),
        "latest_date": str(df['date'].max())[:10],
        "ff_coverage": f"{int(ff_valid / len(df) * 100)}%",
        "schema": get_schema_info(df, SECTOR_FIELD_DESC)
    }

def main():
    stock_res = check_stock_data()
    sector_res = check_sector_data()
    
    report = {
        "generate_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "stock_data": stock_res,
        "sector_data": sector_res
    }
    
    with open(f"{REPORT_DIR}/quality_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
        
    with open(f"{REPORT_DIR}/summary.md", "w", encoding="utf-8") as f:
        f.write(f"## ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š\n**æ—¶é—´**: {report['generate_time']} (UTC)\n\n")
        
        s = report['stock_data']
        f.write(f"### ğŸš€ ä¸ªè‚¡å…¨é‡ (Stock Daily)\n")
        if s.get('status') == 'Success':
            f.write(f"- **Kçº¿è®°å½•æ€»æ•°**: **{s['total_rows']:,}** è¡Œ\n")
            
            ff = s.get('fund_flow')
            if ff:
                f.write(f"- **èµ„é‡‘æµè®°å½•æ•°**: **{ff['valid_count']:,}** è¡Œ\n")
                f.write(f"- **èµ„é‡‘æµå§‹äº**: **{ff['start_date']}**\n")
                f.write(f"- **æ•°æ®å¼‚å¸¸æ•°**: âš ï¸ {ff['anomaly_count']:,} (2010å¹´å‰æˆ–åœç‰Œ)\n")
            
            f.write(f"\n#### ğŸ“‹ å­—æ®µåˆ—è¡¨\n")
            cols = [x['name'] for x in s['schema']]
            f.write(f"`{'`, `'.join(cols)}`\n")
        else:
            f.write(f"âŒ Error: {s.get('message')}\n")
        
        f.write("\n---\n")
        
        sec = report['sector_data']
        f.write(f"### ğŸŒ æ¿å—å…¨é‡ (Sector Full)\n")
        if sec.get('status') == 'Success':
            f.write(f"- **æ€»è®°å½•æ•°**: {sec['total_rows']:,}\n")
            f.write(f"- **æ¿å—æ•°é‡**: {sec['sector_count']}\n")
            f.write(f"- **èµ„é‡‘æµè¦†ç›–ç‡**: **{sec.get('ff_coverage')}**\n")
            
            f.write(f"\n#### ğŸ“‹ å­—æ®µåˆ—è¡¨\n")
            cols = [x['name'] for x in sec['schema']]
            f.write(f"`{'`, `'.join(cols)}`\n")
        else:
            f.write(f"âŒ Error: {sec.get('message')}\n")

if __name__ == "__main__":
    main()
