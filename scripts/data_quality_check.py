# scripts/data_quality_check.py
import pandas as pd
import os
import json
import datetime

# æ ¸å¿ƒæ•°æ®è·¯å¾„
ENGINE_DIR = "final_output/engine"
REPORT_DIR = "final_output/report"
os.makedirs(REPORT_DIR, exist_ok=True)

# ================= å­—æ®µå«ä¹‰å®šä¹‰ (Data Dictionary) =================
STOCK_FIELD_DESC = {
    "date": "äº¤æ˜“æ—¥æœŸ (YYYY-MM-DD)",
    "code": "è‚¡ç¥¨ä»£ç  (e.g. sh.600519)",
    "open": "å¼€ç›˜ä»· (ä¸å¤æƒ)",
    "high": "æœ€é«˜ä»· (ä¸å¤æƒ)",
    "low": "æœ€ä½ä»· (ä¸å¤æƒ)",
    "close": "æ”¶ç›˜ä»· (ä¸å¤æƒ)",
    "volume": "æˆäº¤é‡ (è‚¡)",
    "amount": "æˆäº¤é¢ (å…ƒ)",
    "turn": "æ¢æ‰‹ç‡ (%)",
    "pctChg": "æ¶¨è·Œå¹… (%)",
    "peTTM": "æ»šåŠ¨å¸‚ç›ˆç‡",
    "pbMRQ": "å¸‚å‡€ç‡ (MRQ)",
    "adjustFactor": "åå¤æƒå› å­ (ç”¨äºè®¡ç®—çœŸå®èµ°åŠ¿)",
    "net_flow_amount": "å‡€æµå…¥é‡‘é¢ (å…ƒ)",
    "main_net_flow": "ä¸»åŠ›å‡€æµå…¥ (è¶…å¤§+å¤§å•)",
    "super_large_net_flow": "è¶…å¤§å•å‡€æµå…¥",
    "large_net_flow": "å¤§å•å‡€æµå…¥",
    "medium_small_net_flow": "ä¸­å°å•å‡€æµå…¥"
}

SECTOR_FIELD_DESC = {
    "date": "äº¤æ˜“æ—¥æœŸ",
    "code": "æ¿å—ä»£ç ",
    "name": "æ¿å—åç§°",
    "open": "å¼€ç›˜ç‚¹ä½",
    "close": "æ”¶ç›˜ç‚¹ä½",
    "high": "æœ€é«˜ç‚¹ä½",
    "low": "æœ€ä½ç‚¹ä½",
    "volume": "æˆäº¤é‡ (æ‰‹)",
    "amount": "æˆäº¤é¢ (å…ƒ)",
    "turnover": "æ¢æ‰‹ç‡ (%)",
    "type": "æ¿å—ç±»å‹ (è¡Œä¸š/æ¦‚å¿µ/åœ°åŸŸ)"
}

def get_schema_info(df, desc_map):
    """è·å– DataFrame çš„ Schema ä¿¡æ¯"""
    schema = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        # ç®€åŒ–ç±»å‹æè¿°
        if 'float' in dtype: dtype = 'float'
        elif 'int' in dtype: dtype = 'int'
        elif 'object' in dtype: dtype = 'string'
        
        schema.append({
            "name": col,
            "type": dtype,
            "desc": desc_map.get(col, "è‡ªå®šä¹‰/å…¶ä»–å­—æ®µ")
        })
    return schema

def check_stock_data():
    file_path = f"{ENGINE_DIR}/stock_full.parquet"
    if not os.path.exists(file_path):
        return {"status": "Error", "message": "File not found"}
    
    print(f"ğŸ” æ­£åœ¨æ£€æŸ¥ä¸ªè‚¡å®½è¡¨: {file_path} ...")
    df = pd.read_parquet(file_path)
    total_rows = len(df)
    
    if total_rows == 0:
        return {"status": "Error", "message": "File is empty"}

    # åŸºç¡€æŒ‡æ ‡
    unique_stocks = df['code'].nunique()
    min_date = str(df['date'].min())
    max_date = str(df['date'].max())
    
    # è´¨é‡æŒ‡æ ‡
    missing_flow = df['main_net_flow'].isnull().sum()
    missing_factor = df['adjustFactor'].isnull().sum() if 'adjustFactor' in df.columns else 0
    neg_close = (df['close'] <= 0).sum()
    
    # è¯„åˆ†é€»è¾‘
    score = 100
    if missing_flow / total_rows > 0.5: score -= 20
    if neg_close > 0: score -= 50
    
    return {
        "status": "Success",
        "health_score": score,
        "total_rows": int(total_rows),
        "stock_count": int(unique_stocks),
        "date_range": f"{min_date} ~ {max_date}",
        "missing_fund_flow_pct": round(missing_flow / total_rows * 100, 2),
        "missing_factor_pct": round(missing_factor / total_rows * 100, 2),
        "invalid_price_count": int(neg_close),
        "schema": get_schema_info(df, STOCK_FIELD_DESC) # æ–°å¢ Schema
    }

def check_sector_data():
    full_path = f"{ENGINE_DIR}/sector_full.parquet"
    list_path = f"{ENGINE_DIR}/sector_list.parquet"
    
    if not os.path.exists(full_path):
        return {"status": "Error", "message": "sector_full.parquet not found"}
    
    print(f"ğŸ” æ­£åœ¨æ£€æŸ¥æ¿å—å®½è¡¨: {full_path} ...")
    df = pd.read_parquet(full_path)
    
    # å°è¯•åŠ è½½æ¿å—å…ƒæ•°æ®ä»¥è¿›è¡Œåˆ†ç±»ç»Ÿè®¡
    df_meta = pd.DataFrame()
    if os.path.exists(list_path):
        df_meta = pd.read_parquet(list_path)
    
    total_rows = len(df)
    unique_sectors = df['code'].nunique()
    
    if total_rows == 0:
        return {"status": "Error", "message": "Sector file is empty"}

    # 1. æ—¶æ•ˆæ€§æ£€æŸ¥
    max_date = df['date'].max()
    min_date = df['date'].min()
    latest_count = df[df['date'] == max_date]['code'].nunique()
    miss_update = unique_sectors - latest_count
    
    # 2. é€»è¾‘å®Œæ•´æ€§
    logic_error = (df['high'] < df['low']).sum()
    neg_vol = (df['volume'] < 0).sum()
    
    # 3. åˆ†ç±»ç»Ÿè®¡
    type_stats = {}
    if not df_meta.empty:
        if 'type' in df_meta.columns:
            valid_codes = df['code'].unique()
            valid_meta = df_meta[df_meta['code'].isin(valid_codes)]
            type_counts = valid_meta['type'].value_counts()
            type_stats = type_counts.to_dict()
    
    # 4. å†å²é•¿åº¦ç»Ÿè®¡
    counts = df['code'].value_counts()
    avg_history = int(counts.mean()) if not counts.empty else 0
    
    return {
        "status": "Success",
        "total_rows": int(total_rows),
        "sector_count": int(unique_sectors),
        "date_range": f"{str(min_date)[:10]} ~ {str(max_date)[:10]}",
        "latest_date": str(max_date)[:10],
        "latest_coverage": f"{latest_count}/{unique_sectors}",
        "miss_update_count": int(miss_update),
        "avg_history_days": avg_history,
        "logic_errors": int(logic_error + neg_vol),
        "type_breakdown": type_stats,
        "schema": get_schema_info(df, SECTOR_FIELD_DESC) # æ–°å¢ Schema
    }

def main():
    stock_res = check_stock_data()
    sector_res = check_sector_data()
    
    report = {
        "generate_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "stock_data": stock_res,
        "sector_data": sector_res
    }
    
    # ä¿å­˜ JSON
    json_path = f"{REPORT_DIR}/quality_report.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
        
    # ç”Ÿæˆ Markdown
    md_path = f"{REPORT_DIR}/summary.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(f"## ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š (Data Quality Report)\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {report['generate_time']} (UTC)\n\n")
        
        # --- ä¸ªè‚¡éƒ¨åˆ† ---
        s = report['stock_data']
        f.write(f"### ğŸš€ ä¸ªè‚¡å…¨é‡è¡¨ (Stock Full)\n")
        if s.get('status') == 'Success':
            health_color = "ğŸŸ¢" if s['health_score'] == 100 else "ğŸŸ "
            f.write(f"- **å¥åº·è¯„åˆ†**: {health_color} {s['health_score']} / 100\n")
            f.write(f"- **æ€»è®°å½•æ•°**: {s['total_rows']:,}\n")
            f.write(f"- **è‚¡ç¥¨æ•°é‡**: {s['stock_count']}\n")
            f.write(f"- **æ—¥æœŸèŒƒå›´**: {s['date_range']}\n")
            f.write(f"- **èµ„é‡‘æµç¼ºå¤±ç‡**: {s['missing_fund_flow_pct']}%\n")
            
            # å­—æ®µè¡¨
            f.write(f"\n#### ğŸ“‹ å­—æ®µå­—å…¸\n")
            f.write(f"| å­—æ®µå | ç±»å‹ | è¯´æ˜ |\n")
            f.write(f"| :--- | :--- | :--- |\n")
            for field in s['schema']:
                f.write(f"| `{field['name']}` | {field['type']} | {field['desc']} |\n")
        else:
            f.write(f"âŒ Error: {s.get('message')}\n")
        
        f.write("\n---\n")
        
        # --- æ¿å—éƒ¨åˆ† ---
        sec = report['sector_data']
        f.write(f"### ğŸŒ æ¿å—å…¨é‡è¡¨ (Sector Full)\n")
        if sec.get('status') == 'Success':
            f.write(f"- **æ€»è®°å½•æ•°**: {sec['total_rows']:,}\n")
            f.write(f"- **æ¿å—æ•°é‡**: {sec['sector_count']}\n")
            f.write(f"- **æœ€æ–°æ—¥æœŸ**: **{sec['latest_date']}**\n")
            
            # åˆ†ç±»ç»Ÿè®¡ (å¦‚æœå­˜åœ¨)
            if sec.get('type_breakdown'):
                breakdown_str = ", ".join([f"{k}:{v}" for k, v in sec['type_breakdown'].items()])
                f.write(f"- **åˆ†ç±»ç»Ÿè®¡**: {breakdown_str}\n")

            # å­—æ®µè¡¨
            f.write(f"\n#### ğŸ“‹ å­—æ®µå­—å…¸\n")
            f.write(f"| å­—æ®µå | ç±»å‹ | è¯´æ˜ |\n")
            f.write(f"| :--- | :--- | :--- |\n")
            for field in sec['schema']:
                f.write(f"| `{field['name']}` | {field['type']} | {field['desc']} |\n")
        else:
            f.write(f"âŒ Error: {sec.get('message')}\n")

    print(f"âœ… å¢å¼ºç‰ˆè´¨æ£€æŠ¥å‘Šå·²ç”Ÿæˆ: {json_path}")

if __name__ == "__main__":
    main()
