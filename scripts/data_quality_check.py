# scripts/data_quality_check.py
import pandas as pd
import numpy as np
import os
import json
import datetime

ENGINE_DIR = "final_output/engine"
REPORT_DIR = "final_output/report"
os.makedirs(REPORT_DIR, exist_ok=True)

# ================= æ•°æ®å­—å…¸ =================
STOCK_FIELD_DESC = {
    "date": "äº¤æ˜“æ—¥æœŸ (YYYY-MM-DD)",
    "code": "è‚¡ç¥¨ä»£ç ",
    "close": "æ”¶ç›˜ä»· (åŸå§‹)",
    "peTTM": "æ»šåŠ¨å¸‚ç›ˆç‡",
    "pbMRQ": "å¸‚å‡€ç‡",
    "adjustFactor": "åå¤æƒå› å­",
    "mkt_cap": "æµé€šå¸‚å€¼ (å…ƒ)",
    "volume": "æˆäº¤é‡",
    "turn": "æ¢æ‰‹ç‡",
    "net_flow_amount": "å‡€æµå…¥é‡‘é¢ (å…¨å•, å…ƒ)",
    "main_net_flow": "ä¸»åŠ›å‡€æµå…¥ (è¶…å¤§+å¤§å•, å…ƒ)",
    "super_large_net_flow": "è¶…å¤§å•å‡€æµå…¥",
    "large_net_flow": "å¤§å•å‡€æµå…¥",
    "medium_small_net_flow": "ä¸­å°å•å‡€æµå…¥"
}

SECTOR_FIELD_DESC = {
    "date": "äº¤æ˜“æ—¥æœŸ",
    "code": "æ¿å—ä»£ç ",
    "name": "æ¿å—åç§°",
    "type": "ç±»å‹",
    "close": "æ”¶ç›˜ç‚¹ä½",
    "pctChg": "æ¶¨è·Œå¹…"
}

def get_schema_info(df, desc_map):
    schema = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        if 'float' in dtype: dtype = 'float'
        elif 'int' in dtype: dtype = 'int'
        elif 'object' in dtype: dtype = 'string'
        schema.append({
            "name": col,
            "type": dtype,
            "desc": desc_map.get(col, "è‡ªå®šä¹‰å­—æ®µ")
        })
    return schema

def format_money(val):
    if pd.isna(val): return "N/A"
    abs_val = abs(val)
    if abs_val >= 10**8: return f"{val/10**8:.2f} äº¿"
    elif abs_val >= 10**4: return f"{val/10**4:.2f} ä¸‡"
    else: return f"{val:.2f}"

def check_stock_data():
    file_path = f"{ENGINE_DIR}/stock_full.parquet"
    if not os.path.exists(file_path):
        return {"status": "Error", "message": "File not found"}
    
    print(f"ğŸ” æ£€æŸ¥ä¸ªè‚¡è¡¨: {file_path} ...")
    df = pd.read_parquet(file_path)
    total_rows = len(df)
    
    if total_rows == 0:
        return {"status": "Error", "message": "File is empty"}

    # --- åŸºç¡€æŒ‡æ ‡ ---
    unique_stocks = df['code'].nunique()
    min_date = str(df['date'].min())
    max_date = str(df['date'].max())
    
    # --- èµ„é‡‘æµå‘ä¸“å±è´¨æ£€ ---
    ff_stats = {}
    if 'net_flow_amount' in df.columns:
        # 1. å¼‚å¸¸ç»Ÿè®¡
        nan_count = df['net_flow_amount'].isnull().sum()
        zero_count = (df['net_flow_amount'] == 0).sum()
        anomaly_count = nan_count + zero_count
        
        # 2. ã€æ ¸å¿ƒæ–°å¢ã€‘æ¢æµ‹èµ„é‡‘æµçš„â€œæœ‰æ•ˆèµ·å§‹æ—¥æœŸâ€
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ª net_flow_amount ä¸ä¸º 0 ä¸”ä¸ä¸º NaN çš„æ—¥æœŸ
        valid_ff_df = df[df['net_flow_amount'].notna() & (df['net_flow_amount'] != 0)]
        if not valid_ff_df.empty:
            ff_start_date = str(valid_ff_df['date'].min())
        else:
            ff_start_date = "æ— æœ‰æ•ˆæ•°æ®"

        # 3. è¯„åˆ†é€»è¾‘ (æ”¾å®½å¯¹æ—©æœŸçš„æƒ©ç½š)
        # å¦‚æœå¼‚å¸¸å¤§å¤šé›†ä¸­åœ¨ ff_start_date ä¹‹å‰ï¼Œè¯´æ˜æ˜¯å†å²åŸå› ï¼Œä¸æ˜¯æ•°æ®è´¨é‡é—®é¢˜
        anomaly_rate = anomaly_count / total_rows
        ff_score = max(0, 100 - int(anomaly_rate * 100))
        
        # 4. è¯¦ç»†ç»Ÿè®¡
        pos_flow = (df['net_flow_amount'] > 0).sum()
        neg_flow = (df['net_flow_amount'] < 0).sum()
        max_inflow = df['net_flow_amount'].max()
        max_outflow = df['net_flow_amount'].min()
        
        ff_stats = {
            "score": ff_score,
            "total_rows": int(total_rows),
            "stock_count": int(unique_stocks),
            "date_range": f"{min_date} ~ {max_date}",
            "ff_start_date": ff_start_date, # æ–°å¢å­—æ®µ
            "anomaly_count": int(anomaly_count),
            "details": {
                "nan_rows": int(nan_count),
                "zero_rows": int(zero_count),
                "pos_days": int(pos_flow),
                "neg_days": int(neg_flow),
                "max_in": float(max_inflow),
                "max_out": float(max_outflow)
            }
        }
    
    # --- å…¨å±€æŒ‡æ ‡ ---
    missing_factor = df['adjustFactor'].isnull().sum() if 'adjustFactor' in df.columns else total_rows
    invalid_cap = (df['mkt_cap'] <= 0).sum() if 'mkt_cap' in df.columns else 0
    
    global_score = 100
    if ff_stats.get('score', 0) < 60: global_score -= 20
    if invalid_cap / total_rows > 0.1: global_score -= 10
    
    return {
        "status": "Success",
        "global_score": global_score,
        "total_rows": int(total_rows),
        "stock_count": int(unique_stocks),
        "date_range": f"{min_date} ~ {max_date}",
        "other_metrics": {
            "missing_factor_pct": round(missing_factor / total_rows * 100, 2),
            "invalid_mkt_cap": int(invalid_cap)
        },
        "fund_flow_data": ff_stats,
        "schema": get_schema_info(df, STOCK_FIELD_DESC)
    }

def check_sector_data():
    full_path = f"{ENGINE_DIR}/sector_full.parquet"
    if not os.path.exists(full_path):
        return {"status": "Error", "message": "File not found"}
    
    print(f"ğŸ” æ£€æŸ¥æ¿å—è¡¨: {full_path} ...")
    df = pd.read_parquet(full_path)
    total_rows = len(df)
    
    if total_rows == 0: return {"status": "Error", "message": "Empty"}

    return {
        "status": "Success",
        "total_rows": int(total_rows),
        "sector_count": int(df['code'].nunique()),
        "date_range": f"{str(df['date'].min())[:10]} ~ {str(df['date'].max())[:10]}",
        "latest_date": str(df['date'].max())[:10],
        "schema": get_schema_info(df, SECTOR_FIELD_DESC)
    }

def main():
    stock_res = check_stock_data()
    sector_res = check_sector_data()
    
    report = {
        "generate_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "stock_data": stock_res,
        "sector_data": sector_res
    }
    
    json_path = f"{REPORT_DIR}/quality_report.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
        
    md_path = f"{REPORT_DIR}/summary.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(f"## ğŸ“Š æ•°æ®è´¨é‡æŠ¥å‘Š (Data Quality Report)\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {report['generate_time']} (UTC)\n\n")
        
        # --- Stock Section ---
        s = report['stock_data']
        f.write(f"### ğŸš€ ä¸ªè‚¡å…¨é‡è¡¨ (Stock Full)\n")
        if s.get('status') == 'Success':
            f.write(f"- **å…¨å±€å¥åº·åº¦**: {s['global_score']} / 100\n")
            
            # === èµ„é‡‘æµå‘ä¸“å±åŒºå— ===
            ff = s.get('fund_flow_data', {})
            if ff:
                f.write(f"\n#### ğŸ’° èµ„é‡‘æµå‘è´¨é‡ä¸“åŒº\n")
                score = ff['score']
                icon = "ğŸŸ¢" if score >= 90 else ("ğŸŸ¡" if score >= 60 else "ğŸ”´")
                
                f.write(f"- **èµ„é‡‘æµå¥åº·è¯„åˆ†**: {icon} **{score}** / 100\n")
                f.write(f"- **Kçº¿æ—¥æœŸèŒƒå›´**: {ff['date_range']}\n")
                # ã€æ ¸å¿ƒæ–°å¢ã€‘
                f.write(f"- **èµ„é‡‘æµè¦†ç›–å§‹äº**: **{ff['ff_start_date']}** (åœ¨æ­¤ä¹‹å‰æ— æ•°æ®å±æ­£å¸¸)\n")
                
                anom = ff['anomaly_count']
                f.write(f"- **æ•°æ®å¼‚å¸¸æ•°**: âš ï¸ {anom:,} (æ—©æœŸå†å²ç©ºå€¼ + è¿‘æœŸåœç‰Œ)\n")
                
                det = ff['details']
                f.write(f"\n> **ç»Ÿè®¡ç»†èŠ‚**: å¤šå¤´å¤©æ•° {det['pos_days']:,} | ç©ºå¤´å¤©æ•° {det['neg_days']:,} | å•æ—¥æœ€å¤§æµå…¥ {format_money(det['max_in'])}\n")
            
            om = s.get('other_metrics', {})
            f.write(f"\n#### ğŸ›  å…¶ä»–æŒ‡æ ‡\n")
            f.write(f"- å¸‚å€¼å¼‚å¸¸è®°å½•(<=0): {om.get('invalid_mkt_cap')}\n")
            f.write(f"- å¤æƒå› å­ç¼ºå¤±ç‡: {om.get('missing_factor_pct')}%\n")

            f.write(f"\n#### ğŸ“‹ å­—æ®µå­—å…¸\n| å­—æ®µ | ç±»å‹ | è¯´æ˜ |\n|---|---|---|\n")
            for field in s['schema']:
                f.write(f"| `{field['name']}` | {field['type']} | {field['desc']} |\n")
        else:
            f.write(f"âŒ Error: {s.get('message')}\n")
        
        f.write("\n---\n")
        
        # --- Sector Section ---
        sec = report['sector_data']
        f.write(f"### ğŸŒ æ¿å—å…¨é‡è¡¨ (Sector Full)\n")
        if sec.get('status') == 'Success':
            f.write(f"- **æ¿å—æ•°é‡**: {sec['sector_count']}\n")
            f.write(f"- **æ€»è®°å½•æ•°**: {sec['total_rows']:,}\n")
            f.write(f"- **æœ€æ–°æ—¥æœŸ**: **{sec['latest_date']}**\n")
            f.write(f"\n#### ğŸ“‹ å­—æ®µå­—å…¸\n| å­—æ®µ | ç±»å‹ | è¯´æ˜ |\n|---|---|---|\n")
            for field in sec['schema']:
                f.write(f"| `{field['name']}` | {field['type']} | {field['desc']} |\n")
        else:
            f.write(f"âŒ Error: {sec.get('message')}\n")

    print(f"âœ… è´¨æ£€æŠ¥å‘Šå·²ç”Ÿæˆ: {json_path}")

if __name__ == "__main__":
    main()
