# scripts/download_sector.py
import requests
import pandas as pd
import time
import random
import os
import sys
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

OUTPUT_DIR = "final_output/engine"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å°è¯•è·å– Cloudflare Worker ç¯å¢ƒå˜é‡
CF_WORKER_URL = os.getenv("CF_WORKER_URL")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "http://quote.eastmoney.com/",
    "Connection": "close"
}

def create_session():
    """åˆ›å»ºä¸€ä¸ªé«˜å¯ç”¨çš„ Session"""
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504, 104])
    session.mount('http://', HTTPAdapter(max_retries=retries))
    session.mount('https://', HTTPAdapter(max_retries=retries))
    session.headers.update(HEADERS)
    return session

sess = create_session()

def get_sector_list_raw(name, fs):
    """è·å–æ¿å—åˆ—è¡¨"""
    sectors = []
    page = 1
    page_size = 100
    base_url = "http://17.push2.eastmoney.com/api/qt/clist/get"
    
    print(f"æ­£åœ¨è·å– {name} åˆ—è¡¨...", end="", flush=True)
    
    while True:
        success = False
        res_json = None
        for retry in range(3):
            params = {
                "pn": page, "pz": page_size, "po": 1, "np": 1, 
                "ut": "bd1d9ddb04089700cf9c27f6f7426281",
                "fltt": 2, "invt": 2, "fid": "f3", "fs": fs,
                "fields": "f12,f13,f14" 
            }
            try:
                if CF_WORKER_URL:
                    params["target_func"] = "list"
                    resp = sess.get(CF_WORKER_URL, params=params, timeout=30)
                else:
                    resp = sess.get(base_url, params=params, timeout=10)
                res_json = resp.json()
                success = True
                break
            except:
                time.sleep(1)
        
        if not success:
            print(f" [Page {page} Failed] ", end="")
            break
            
        try:
            if res_json and res_json.get('data') and res_json['data'].get('diff'):
                data = res_json['data']['diff']
                for item in data:
                    item['type'] = name
                sectors.extend(data)
                print(".", end="", flush=True)
                if len(data) < page_size: break
                page += 1
                time.sleep(0.1)
            else:
                break
        except: break
            
    print(f" -> {len(sectors)} ä¸ª")
    return sectors

def get_sector_list():
    all_sectors = []
    targets = {"è¡Œä¸š": "m:90 t:2", "æ¦‚å¿µ": "m:90 t:3", "åœ°åŸŸ": "m:90 t:1"}
    for name, fs in targets.items():
        data = get_sector_list_raw(name, fs)
        all_sectors.extend(data)
    
    df = pd.DataFrame(all_sectors)
    if df.empty: return pd.DataFrame()
    return df.rename(columns={'f12': 'code', 'f13': 'market', 'f14': 'name'})

def get_kline_history(secid, clean_code):
    """è·å– K çº¿å†å²"""
    params = {
        "secid": secid,
        "fields1": "f1,f2,f3,f4,f5,f6",
        "fields2": "f51,f52,f53,f54,f55,f56,f57,f58", # æ—¥æœŸ,å¼€,æ”¶,é«˜,ä½,é‡,é¢,æ¢æ‰‹
        "klt": "101", "fqt": "1", "beg": "19900101", "end": "20500101", "lmt": "1000000"
    }
    
    try:
        if CF_WORKER_URL:
            params["target_func"] = "kline"
            res = sess.get(CF_WORKER_URL, params=params, timeout=30).json()
        else:
            url = "http://push2his.eastmoney.com/api/qt/stock/kline/get"
            res = sess.get(url, params=params, timeout=10).json()
        
        if res and res.get('data') and res['data'].get('klines'):
            klines = res['data']['klines']
            data = [x.split(',') for x in klines]
            df = pd.DataFrame(data, columns=['date', 'open', 'close', 'high', 'low', 'volume', 'amount', 'turnover'])
            df['code'] = clean_code
            cols = ['open', 'close', 'high', 'low', 'volume', 'amount', 'turnover']
            df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')
            return df
    except Exception as e:
        # print(f"Kline err {clean_code}: {e}")
        pass
    return pd.DataFrame()

def get_flow_history(secid, clean_code):
    """ã€æ–°å¢ã€‘è·å–èµ„é‡‘æµå†å²"""
    # å­—æ®µæ˜ å°„ï¼š
    # f51:æ—¥æœŸ, f52:ä¸»åŠ›å‡€æµå…¥, f53:å°å•, f54:ä¸­å•, f55:å¤§å•, f56:è¶…å¤§å•
    params = {
        "secid": secid,
        "fields1": "f1,f2,f3,f7",
        "fields2": "f51,f52,f53,f54,f55,f56", 
        "klt": "101", "lmt": "0" # 0ä»£è¡¨å…¨é‡
    }
    
    try:
        if CF_WORKER_URL:
            params["target_func"] = "flow" # è°ƒç”¨ Worker çš„ flow æ¥å£
            res = sess.get(CF_WORKER_URL, params=params, timeout=30).json()
        else:
            url = "http://push2his.eastmoney.com/api/qt/stock/fflow/daykline/get"
            res = sess.get(url, params=params, timeout=10).json()
            
        if res and res.get('data') and res['data'].get('klines'):
            klines = res['data']['klines']
            data = [x.split(',') for x in klines]
            df = pd.DataFrame(data, columns=['date', 'main_net_flow', 'small_net_flow', 'medium_net_flow', 'large_net_flow', 'super_large_net_flow'])
            
            # è®¡ç®— net_flow_amount (ä¸»åŠ› = è¶…å¤§+å¤§)
            # ä¸œè´¢æ¥å£é‡Œ f52 å·²ç»æ˜¯ä¸»åŠ›å‡€æµå…¥
            df.rename(columns={'main_net_flow': 'net_flow_amount'}, inplace=True) 
            # å…¼å®¹æ€§ï¼šè¿™é‡Œæˆ‘ä»¬å°† net_flow_amount è§†ä¸ºä¸»åŠ›å‡€æµå…¥ï¼Œä¸ä¸ªè‚¡ä¿æŒä¸€è‡´
            # ä¸ªè‚¡è¡¨ä¸­ net_flow_amount æ˜¯å…¨å•å‡€æµå…¥å—ï¼Ÿé€šå¸¸ä¸»åŠ›å‡€æµå…¥æ›´æœ‰ä»·å€¼ã€‚
            # ä¸ºäº†ç»Ÿä¸€ï¼Œæˆ‘ä»¬æŠŠ f52 æ˜ å°„ä¸º main_net_flow
            
            df['code'] = clean_code
            cols = ['net_flow_amount', 'small_net_flow', 'medium_net_flow', 'large_net_flow', 'super_large_net_flow']
            df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')
            
            # è¿™é‡Œé¢å¤–ç”Ÿæˆä¸€ä¸ª main_net_flow å­—æ®µï¼Œç­‰äº net_flow_amount (ä¸œè´¢å®šä¹‰f52å³ä¸»åŠ›)
            df['main_net_flow'] = df['net_flow_amount']
            
            return df
    except Exception as e:
        # print(f"Flow err {clean_code}: {e}")
        pass
    return pd.DataFrame()

def process_one_sector(code, market):
    clean_code = str(code)
    # æ„é€  secid
    if str(market) == '90' and not clean_code.startswith('BK'):
        secid = f"{market}.BK{clean_code}"
    else:
        secid = f"{market}.{clean_code}"
        
    # 1. ä¸‹è½½ K çº¿
    df_k = get_kline_history(secid, clean_code)
    
    # 2. å¤‡ç”¨ secid å°è¯• (å¤„ç† BK å‰ç¼€ä¸ä¸€è‡´)
    if df_k.empty and ".BK" in secid:
        alt_secid = secid.replace(".BK", ".")
        df_k = get_kline_history(alt_secid, clean_code)
        if not df_k.empty:
            secid = alt_secid # ä¿®æ­£ secid ç”¨äºåç»­èµ„é‡‘æµä¸‹è½½

    if df_k.empty: return pd.DataFrame()

    # 3. ä¸‹è½½ èµ„é‡‘æµ
    df_f = get_flow_history(secid, clean_code)
    
    # 4. åˆå¹¶ (Left Join)
    if not df_f.empty:
        df_merged = pd.merge(df_k, df_f, on=['date', 'code'], how='left')
        # å¡«å…… NaN ä¸º 0 (æ—©æœŸæ²¡æœ‰èµ„é‡‘æµæ•°æ®)
        flow_cols = ['net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_net_flow', 'small_net_flow']
        for c in flow_cols:
            if c in df_merged.columns:
                df_merged[c] = df_merged[c].fillna(0)
        return df_merged
    
    return df_k

def main():
    if CF_WORKER_URL:
        print(f"ğŸš€ ä»£ç†æ¨¡å¼: {CF_WORKER_URL}")
    else:
        print("ğŸ¢ ç›´è¿æ¨¡å¼")

    # 1. è·å–åˆ—è¡¨
    print("Step 1: è·å–å…¨å¸‚åœºæ¿å—åˆ—è¡¨...")
    df_list = get_sector_list()
    if df_list.empty:
        print("âŒ åˆ—è¡¨è·å–å¤±è´¥")
        return
    
    df_list.drop_duplicates(subset=['code'], inplace=True)
    unique_count = len(df_list)
    print(f"âœ… æœ€ç»ˆæœ‰æ•ˆç›®æ ‡: {unique_count} ä¸ª")
    
    df_list.to_parquet(f"{OUTPUT_DIR}/sector_list.parquet", index=False)
    
    # 2. å¾ªç¯è¡¥å½•
    all_dfs = []
    downloaded_codes = set()
    MAX_ROUNDS = 3
    
    for round_num in range(1, MAX_ROUNDS + 1):
        pending_df = df_list[~df_list['code'].isin(downloaded_codes)]
        if pending_df.empty:
            print("âœ¨ æ‰€æœ‰æ¿å—å·²å…¨éƒ¨ä¸‹è½½å®Œæˆï¼")
            break
            
        print(f"\nğŸ”„ ç¬¬ {round_num}/{MAX_ROUNDS} è½®ä¸‹è½½ (å‰©ä½™ {len(pending_df)} ä¸ª)...")
        
        count = 0
        for _, row in pending_df.iterrows():
            # åŒæ—¶ä¸‹è½½ Kçº¿ + èµ„é‡‘æµ
            df = process_one_sector(row['code'], row['market'])
            
            if not df.empty:
                all_dfs.append(df)
                downloaded_codes.add(row['code'])
            
            count += 1
            if count % 50 == 0:
                print(f"   è¿›åº¦: {count}/{len(pending_df)} | æˆåŠŸ: {len(downloaded_codes)}")
            
            time.sleep(0.05)
    
    # 3. åˆå¹¶ä¿å­˜
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡: ç›®æ ‡ {unique_count} -> æˆåŠŸ {len(downloaded_codes)}")
    
    if all_dfs:
        print("æ­£åœ¨åˆå¹¶å®½è¡¨...")
        full_df = pd.concat(all_dfs, ignore_index=True)
        full_df.sort_values(['code', 'date'], inplace=True)
        
        # å‹ç¼©ç±»å‹
        float_cols = full_df.select_dtypes(include=['float64']).columns
        full_df[float_cols] = full_df[float_cols].astype('float32')
        
        outfile = f"{OUTPUT_DIR}/sector_full.parquet"
        full_df.to_parquet(outfile, index=False, compression='zstd')
        print(f"âœ… æ–‡ä»¶å·²ç”Ÿæˆ: {outfile}")
        print(f"   æ€»è®°å½•æ•°: {len(full_df)}")
        print(f"   åŒ…å«èµ„é‡‘æµåˆ—: {'net_flow_amount' in full_df.columns}")
    else:
        print("âŒ ä¸¥é‡é”™è¯¯ï¼šæœªä¸‹è½½åˆ°æ•°æ®ï¼")

if __name__ == "__main__":
    main()
