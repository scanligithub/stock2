# scripts/merge_data.py
import duckdb
import pandas as pd
import pandas_ta as ta
import numpy as np
import os
import glob
import datetime
import pyarrow as pa
import pyarrow.parquet as pq
from tqdm import tqdm
import gc

# === è·¯å¾„é…ç½® ===
CACHE_DIR = "cache_data"          
KLINE_DIR = "downloaded_kline"    
FLOW_DIR = "downloaded_fundflow"  

# è¾“å‡ºç›®å½•
OUTPUT_ENGINE = "final_output/engine"
OUTPUT_DAILY = f"{OUTPUT_ENGINE}/stock_daily" # å­˜æ”¾ OSS ç”¨çš„å½“å¹´æ–‡ä»¶
CACHE_OUTPUT_FILE = f"{CACHE_DIR}/stock_buffer.parquet" # å­˜æ”¾ç»™æ˜å¤©ç”¨çš„å…¨é‡Buffer

os.makedirs(OUTPUT_DAILY, exist_ok=True)

# === DuckDB åˆå§‹åŒ– ===
# é™åˆ¶ DuckDB ä½¿ç”¨å†…å­˜ä¸Šé™ï¼Œç»™ Python é¢„ç•™ç©ºé—´
con = duckdb.connect(database=':memory:')
con.execute("SET memory_limit='4GB';") 
con.execute("SET threads=2;") # é™åˆ¶çº¿ç¨‹æ•°ï¼Œé¿å… GHA èµ„æºäº‰æŠ¢

def get_files_list():
    """è·å–æ‰€æœ‰éœ€è¦åˆå¹¶çš„æ–‡ä»¶åˆ—è¡¨"""
    history_files = glob.glob(f"{CACHE_DIR}/*.parquet")
    kline_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    
    # èµ„é‡‘æµæ–‡ä»¶åšæˆå­—å…¸ï¼Œæ–¹ä¾¿ SQL å…³è” (å¦‚æœ DuckDB ç›´æ¥å…³è”å¤ªæ…¢ï¼Œæˆ‘ä»¬åœ¨ Python é‡Œåš)
    # è¿™é‡Œç­–ç•¥ï¼šèµ„é‡‘æµå…ˆä¸è¿› DuckDB è§†å›¾ï¼Œåœ¨ Python å¤„ç†å•åªè‚¡ç¥¨æ—¶å† mergeï¼Œè¿™æ ·æœ€ç¨³
    flow_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    flow_map = {os.path.basename(f): f for f in flow_files}
    
    return history_files + kline_files, flow_map

def calculate_indicators(df):
    """
    è®¡ç®—å•åªè‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡
    df å·²ç»åŒ…å«äº† full history
    """
    # 1. ä»·æ ¼å‡çº¿
    for w in [5, 10, 20, 60, 120, 250]:
        df[f'ma{w}'] = df['close'].rolling(window=w).mean()
    
    # 2. æˆäº¤é‡å‡çº¿
    for w in [5, 10, 20, 30]:
        df[f'vol_ma{w}'] = df['volume'].rolling(window=w).mean()

    # 3. MACD
    try:
        macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
        if macd is not None:
            df['dif'] = macd.iloc[:, 0]
            df['macd'] = macd.iloc[:, 1]
            df['dea'] = macd.iloc[:, 2]
    except: pass

    # 4. KDJ
    try:
        kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
        if kdj is not None:
            df['k'] = kdj.iloc[:, 0]
            df['d'] = kdj.iloc[:, 1]
            df['j'] = kdj.iloc[:, 2]
    except: pass

    # 5. RSI
    try:
        df['rsi6'] = df.ta.rsi(close='close', length=6)
        df['rsi12'] = df.ta.rsi(close='close', length=12)
        df['rsi24'] = df.ta.rsi(close='close', length=24)
    except: pass

    # 6. BOLL
    try:
        boll = df.ta.bbands(close='close', length=20, std=2)
        if boll is not None:
            df['boll_lb'] = boll.iloc[:, 0]
            df['boll_up'] = boll.iloc[:, 2]
    except: pass

    # 7. å…¶ä»–
    try:
        df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
        df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
    except: pass

    return df

def process_resample(writer_w, writer_m, df_daily):
    """
    æµå¼ç”Ÿæˆå‘¨/æœˆçº¿
    æ³¨æ„ï¼šè¿™é‡Œçš„ df_daily åªæ˜¯å•åªè‚¡ç¥¨çš„æ•°æ®ï¼Œå†…å­˜å¾ˆå°
    """
    if df_daily.empty: return

    # èšåˆè§„åˆ™
    agg_rules = {
        'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
        'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
        'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
    }
    # èµ„é‡‘æµå­—æ®µç´¯åŠ 
    flow_cols = [c for c in df_daily.columns if 'flow' in c]
    for c in flow_cols: agg_rules[c] = 'sum'

    # ç”Ÿæˆå‘¨çº¿
    try:
        df_w = df_daily.set_index('date').resample('W-FRI').agg(agg_rules).dropna(subset=['close']).reset_index()
        # ç®€å•è®¡ç®—å‘¨çº¿å‡çº¿
        if not df_w.empty:
            df_w['code'] = df_daily['code'].iloc[0]
            for w in [5, 10, 20]:
                df_w[f'ma{w}'] = df_w['close'].rolling(w).mean()
            
            # å†™å…¥æµ
            table_w = pa.Table.from_pandas(df_w)
            writer_w.write_table(table_w)
    except: pass

    # ç”Ÿæˆæœˆçº¿
    try:
        df_m = df_daily.set_index('date').resample('ME').agg(agg_rules).dropna(subset=['close']).reset_index()
        if not df_m.empty:
            df_m['code'] = df_daily['code'].iloc[0]
            for w in [5, 10, 20]:
                df_m[f'ma{w}'] = df_m['close'].rolling(w).mean()
            
            table_m = pa.Table.from_pandas(df_m)
            writer_m.write_table(table_m)
    except: pass

def main():
    print("ğŸš€ å¼€å§‹ DuckDB æµå¼åˆå¹¶ä¸è®¡ç®—...")
    
    # 1. å‡†å¤‡æ–‡ä»¶åˆ—è¡¨
    input_files, flow_map = get_files_list()
    if not input_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• K çº¿æ–‡ä»¶")
        return

    # 2. åœ¨ DuckDB ä¸­å»ºç«‹ç»Ÿä¸€è§†å›¾ (Virtual View)
    # ä½¿ç”¨ list ä¼ é€’æ–‡ä»¶è·¯å¾„ï¼ŒDuckDB ä¼šè‡ªåŠ¨å¤„ç† union
    # è¿™ä¸€æ­¥æ˜¯é›¶æ‹·è´çš„ï¼Œä¸ä¼šè¯»å–æ•°æ®åˆ°å†…å­˜
    print(f"ğŸ”— æ­£åœ¨å»ºç«‹ {len(input_files)} ä¸ªæ–‡ä»¶çš„è™šæ‹Ÿè§†å›¾...")
    
    # å°†æ–‡ä»¶åˆ—è¡¨è½¬æ¢ä¸º SQL å­—ç¬¦ä¸²åˆ—è¡¨ ['file1', 'file2']
    files_sql = str(input_files).replace('[', '').replace(']', '')
    
    # åˆ›å»ºè§†å›¾ï¼šåˆå¹¶ + å»é‡ + æ’åº
    # QUALIFY row_number... ç”¨äºå»é‡ (ä¿ç•™æœ€æ–°çš„è®°å½•)
    con.execute(f"""
        CREATE OR REPLACE VIEW raw_kline AS 
        SELECT * FROM read_parquet({input_files})
    """)
    
    # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç  (Distinct)
    print("ğŸ” æ‰«ææ‰€æœ‰è‚¡ç¥¨ä»£ç ...")
    codes_df = con.execute("SELECT DISTINCT code FROM raw_kline ORDER BY code").fetchdf()
    all_codes = codes_df['code'].tolist()
    print(f"âœ… å‘ç° {len(all_codes)} åªè‚¡ç¥¨")

    # 3. åˆå§‹åŒ– Parquet Writers (æµå¼å†™å…¥å™¨)
    # æˆ‘ä»¬éœ€è¦åŒæ—¶å†™å‡ºï¼š
    # A. ç¼“å­˜æ–‡ä»¶ (Full History)
    # B. OSS å½“å¹´æ–‡ä»¶ (Current Year)
    # C. å‘¨çº¿/æœˆçº¿æ–‡ä»¶
    
    current_year = datetime.datetime.now().year
    
    # å®šä¹‰ Schema å ä½ç¬¦ (å…ˆè¯»å–ç¬¬ä¸€åªè‚¡ç¥¨æ¥ç¡®å®š Schema)
    first_code = all_codes[0]
    df_sample = con.execute(f"SELECT * FROM raw_kline WHERE code='{first_code}'").fetchdf()
    # æ¨¡æ‹Ÿè®¡ç®—ä¸€æ¬¡ä»¥è·å–æœ€ç»ˆ Schema (åŒ…å« MA, MACD ç­‰åˆ—)
    df_sample['date'] = pd.to_datetime(df_sample['date'])
    df_sample = calculate_indicators(df_sample)
    
    # ç»Ÿä¸€è½¬ float32
    float_cols = df_sample.select_dtypes(include=['float64']).columns
    df_sample[float_cols] = df_sample[float_cols].astype('float32')
    
    schema_full = pa.Table.from_pandas(df_sample).schema
    # å‘¨æœˆçº¿ Schema ç¨æœ‰ä¸åŒï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåœ¨ process_resample å†…éƒ¨å¤„ç†ï¼Œæš‚æ—¶ä¸é¢„å®šä¹‰ writer schema
    # ä¸ºäº†ç®€å•ï¼Œå‘¨æœˆçº¿æˆ‘ä»¬ç”¨ append æ¨¡å¼ï¼Œæˆ–è€…æœ€ååˆå¹¶ã€‚
    # é‰´äºå‘¨æœˆçº¿æ•°æ®é‡å°ï¼Œæˆ‘ä»¬å¯ä»¥æš‚å­˜åœ¨å†…å­˜ list ä¸­ï¼Œæœ€åä¸€æ¬¡æ€§å†™å‡º
    
    # æ‰“å¼€æµå¼å†™å…¥å™¨
    writer_buffer = pq.ParquetWriter(CACHE_OUTPUT_FILE, schema_full, compression='zstd')
    
    # OSS æ–‡ä»¶è·¯å¾„
    oss_file = f"{OUTPUT_DAILY}/stock_{current_year}.parquet"
    writer_oss = pq.ParquetWriter(oss_file, schema_full, compression='zstd')
    
    # å‘¨æœˆçº¿æš‚æ—¶ç”¨å†…å­˜ç¼“å­˜ (å› ä¸ºå®ƒä»¬è¿˜è¦ resampleï¼Œä¸”ä½“ç§¯å°)
    # 300MB çš„å‘¨çº¿å¦‚æœåˆ†ç‰‡å†™å¯èƒ½ä¼šå¯¼è‡´ footer å¼€é”€å¤§ï¼Œè¿™é‡Œæˆ‘ä»¬å…ˆæ”¶é›† buffer
    # å¦‚æœå‘¨çº¿ä¹Ÿ OOMï¼Œé‚£å°±ä¹Ÿå¾—å¼€ Writerã€‚è€ƒè™‘åˆ° GitHub 7GB å†…å­˜ï¼Œå‘¨çº¿å…¨é‡æ‰ 300MBï¼Œå¯ä»¥æ”¾å†…å­˜ã€‚
    # ä¸ºäº†æè‡´å®‰å…¨ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”¨ Writer å§ã€‚
    # éœ€è¦å…ˆæ¨æ–­å‘¨çº¿ Schema... æ¯”è¾ƒéº»çƒ¦ã€‚
    # æ–¹æ¡ˆï¼šå‘¨æœˆçº¿å•ç‹¬å¤„ç†ï¼Œå…ˆç”Ÿæˆä¸´æ—¶æ–‡ä»¶ï¼Œæœ€ååˆå¹¶ã€‚
    
    weekly_buffer = []
    monthly_buffer = []

    # 4. ğŸš€ å¼€å§‹æµå¼å¤„ç† (Loop by Code)
    print("ğŸŒŠ å¼€å§‹æµå¼å¤„ç† (Reading -> Calc -> Writing)...")
    
    # é¢„ç¼–è¯‘ SQL æå‡æ€§èƒ½
    # DuckDB çš„ prepare è¯­å¥åœ¨ Python API ä¸­ä¸ç›´æ¥æ”¯æŒå¸¦å‚æ•°çš„ DF è¿”å›ï¼Œç›´æ¥ç”¨ f-string å³å¯ï¼ŒDuckDBè§£æå¾ˆå¿«
    
    for code in tqdm(all_codes):
        # A. ä» DuckDB è¯»å–å•åªè‚¡ç¥¨çš„å…¨é‡å†å² (å†…å­˜å ç”¨æå°)
        # å¼ºåˆ¶æŒ‰æ—¥æœŸæ’åº
        sql = f"SELECT * FROM raw_kline WHERE code='{code}' ORDER BY date"
        df = con.execute(sql).fetchdf()
        
        # B. å…³è”èµ„é‡‘æµ (Pandas Merge)
        # æ–‡ä»¶ååŒ¹é…
        fname = f"{code}.parquet"
        if fname in flow_map:
            try:
                df_flow = pd.read_parquet(flow_map[fname])
                if not df_flow.empty:
                    df['date'] = pd.to_datetime(df['date'])
                    df_flow['date'] = pd.to_datetime(df_flow['date'])
                    # Left Join
                    df = pd.merge(df, df_flow, on=['date', 'code'], how='left')
            except: pass
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼
        if df['date'].dtype == 'object':
            df['date'] = pd.to_datetime(df['date'])

        # C. è®¡ç®—æŒ‡æ ‡ (Pandas/TA)
        df = calculate_indicators(df)
        
        # D. ç”Ÿæˆå‘¨/æœˆçº¿ (Resample)
        process_resample(None, None, df) # ä¸´æ—¶ç¦ç”¨ Writerï¼Œæ”¹ä¸ºæ”¶é›†
        # ä¿®æ”¹ process_resample é€»è¾‘ä½¿å…¶è¿”å› dfï¼Œè€Œä¸æ˜¯ write
        # è¿™é‡Œä¸ºäº†ä¸ç ´åç»“æ„ï¼Œæˆ‘ä»¬æŠŠèšåˆé€»è¾‘æå–å‡ºæ¥
        
        # --- å‘¨/æœˆçº¿ æ”¶é›†é€»è¾‘ (å†…åµŒ) ---
        agg_rules = {
            'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
            'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
            'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
        }
        for c in ['net_flow_amount', 'main_net_flow']: 
            if c in df.columns: agg_rules[c] = 'sum'
            
        # å‘¨çº¿
        try:
            w_df = df.set_index('date').resample('W-FRI').agg(agg_rules).dropna(subset=['close']).reset_index()
            w_df['code'] = code
            weekly_buffer.append(w_df)
        except: pass
        
        # æœˆçº¿
        try:
            m_df = df.set_index('date').resample('ME').agg(agg_rules).dropna(subset=['close']).reset_index()
            m_df['code'] = code
            monthly_buffer.append(m_df)
        except: pass
        # -----------------------------

        # E. æ•°æ®ç±»å‹ä¼˜åŒ– (å‡†å¤‡å†™å…¥æ—¥çº¿)
        float_cols = df.select_dtypes(include=['float64']).columns
        df[float_cols] = df[float_cols].astype('float32')
        df['date'] = df['date'].dt.strftime('%Y-%m-%d')
        
        # F. å†™å…¥ Full Buffer (ç»™ç¼“å­˜ç”¨)
        # éœ€è¦å¯¹é½åˆ—åï¼Œé˜²æ­¢èµ„é‡‘æµåˆ—æœ‰çš„è‚¡ç¥¨æœ‰ï¼Œæœ‰çš„æ²¡æœ‰
        # è¡¥é½ç¼ºå¤±åˆ—
        for col in schema_full.names:
            if col not in df.columns:
                df[col] = np.nan
        
        # æŒ‰ç…§ Schema é¡ºåºé‡æ’
        df = df[schema_full.names]
        
        table = pa.Table.from_pandas(df, schema=schema_full)
        writer_buffer.write_table(table)
        
        # G. å†™å…¥ OSS Current Year (ä»…ä»Šå¹´)
        df_curr = df[df['date'] >= f"{current_year}-01-01"]
        if not df_curr.empty:
            table_curr = pa.Table.from_pandas(df_curr, schema=schema_full)
            writer_oss.write_table(table_curr)
            
        # H. æ˜¾å¼åƒåœ¾å›æ”¶ (éå¸¸é‡è¦)
        del df, table
        # æ¯å¤„ç† 100 åªè‚¡ç¥¨ GC ä¸€æ¬¡ï¼Œå¹³è¡¡é€Ÿåº¦ä¸å†…å­˜
        # if i % 100 == 0: gc.collect() 

    # 5. å…³é—­ Writers
    writer_buffer.close()
    writer_oss.close()
    print("âœ… æ—¥çº¿æ•°æ®å¤„ç†å®Œæˆ (Buffer + OSS)")

    # 6. ç»Ÿä¸€ä¿å­˜å‘¨/æœˆçº¿
    # å› ä¸ºå‘¨æœˆçº¿æ•°æ®é‡å° (å‘¨çº¿~300MB, æœˆçº¿~80MB)ï¼Œconcat æ²¡é—®é¢˜
    print("ğŸ“… ä¿å­˜å‘¨/æœˆçº¿æ•°æ®...")
    if weekly_buffer:
        df_w = pd.concat(weekly_buffer, ignore_index=True)
        # è®¡ç®—å‘¨çº¿æŒ‡æ ‡ (å…¨é‡ç®—æ›´å¿«)
        df_w = df_w.groupby('code', group_keys=False).apply(lambda x: calculate_indicators(x.sort_values('date')))
        # å‹ç¼©ä¿å­˜
        float_cols = df_w.select_dtypes(include=['float64']).columns
        df_w[float_cols] = df_w[float_cols].astype('float32')
        df_w['date'] = df_w['date'].dt.strftime('%Y-%m-%d')
        df_w.to_parquet(f"{OUTPUT_ENGINE}/stock_weekly.parquet", index=False, compression='zstd')
    
    if monthly_buffer:
        df_m = pd.concat(monthly_buffer, ignore_index=True)
        df_m = df_m.groupby('code', group_keys=False).apply(lambda x: calculate_indicators(x.sort_values('date')))
        float_cols = df_m.select_dtypes(include=['float64']).columns
        df_m[float_cols] = df_m[float_cols].astype('float32')
        df_m['date'] = df_m['date'].dt.strftime('%Y-%m-%d')
        df_m.to_parquet(f"{OUTPUT_ENGINE}/stock_monthly.parquet", index=False, compression='zstd')

    print("ğŸ‰ å…¨æµç¨‹ç»“æŸï¼")

if __name__ == "__main__":
    main()
