# scripts/merge_data.py
import pandas as pd
import glob
import os
from tqdm import tqdm
import pandas_ta as ta
import numpy as np
import duckdb
import shutil
import datetime

# --- é…ç½®è·¯å¾„ ---
KLINE_DIR = "downloaded_kline" 
FLOW_DIR = "downloaded_fundflow"
CACHE_DIR = "cache_data" 
OUTPUT_BASE = "final_output/engine"

# è¾“å‡ºç›®å½•ç»“æ„
DIRS = {
    "daily": f"{OUTPUT_BASE}/stock_daily",
    "weekly": f"{OUTPUT_BASE}", # å‘¨çº¿æ”¾åœ¨ engine æ ¹ç›®å½•
    "monthly": f"{OUTPUT_BASE}" # æœˆçº¿æ”¾åœ¨ engine æ ¹ç›®å½•
}
for d in DIRS.values(): os.makedirs(d, exist_ok=True)

# å†å²å½’æ¡£æ–‡ä»¶å (å†·æ•°æ®ï¼Œä» Release ä¸‹è½½çš„)
ARCHIVE_FILENAME = "stock_history_2005_2024.parquet"
ARCHIVE_PATH = f"{CACHE_DIR}/{ARCHIVE_FILENAME}"

def clean_indicators(df):
    """
    å¼ºåˆ¶ç±»å‹æ¸…æ´—ï¼šå°†æ‰€æœ‰æ•°å€¼åˆ—è½¬ä¸º float32 ä»¥èŠ‚çœç©ºé—´ï¼Œ
    å¹¶å°†è®¡ç®—å¤±è´¥äº§ç”Ÿçš„æ— æ•ˆå€¼(Object/Timestamp)è½¬ä¸º NaN
    """
    target_cols = [
        # MACD
        'dif', 'dea', 'macd', 
        # KDJ
        'k', 'd', 'j',
        # RSI
        'rsi6', 'rsi12', 'rsi24',
        # BOLL
        'boll_up', 'boll_lb',
        # Other
        'cci', 'atr',
        # Funds
        'net_flow_amount', 'main_net_flow', 'super_large_net_flow', 
        'large_net_flow', 'medium_small_net_flow',
        # Factors & Basic
        'peTTM', 'pbMRQ', 'adjustFactor', 'mkt_cap',
        'open', 'high', 'low', 'close', 'volume', 'amount', 'turn', 'pctChg',
        # MAs
        'vol_ma5', 'vol_ma10', 'vol_ma20', 'vol_ma30'
    ]
    for w in [5, 10, 20, 60, 120, 250]: target_cols.append(f'ma{w}')

    for col in target_cols:
        if col in df.columns:
            # errors='coerce' ä¼šæŠŠæ— æ³•è½¬æ¢çš„æ•°æ®å˜æˆ NaN
            df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')
    return df

def calculate_all_indicators(df):
    """
    è®¡ç®—å•åªè‚¡ç¥¨çš„å…¨å¥—æŒ‡æ ‡
    æ³¨æ„ï¼šä¼ å…¥çš„ df å¿…é¡»æ˜¯æŒ‰æ—¥æœŸå‡åºæ’åˆ—çš„
    """
    # ç¡®ä¿æ’åº
    df = df.sort_values('date').reset_index(drop=True)
    
    # 1. ä»·æ ¼å‡çº¿ (ä½¿ç”¨ Pandas Rolling åŸç”Ÿè®¡ç®—ï¼Œé€Ÿåº¦å¿«)
    for w in [5, 10, 20, 60, 120, 250]:
        df[f'ma{w}'] = df['close'].rolling(w).mean()
    
    # 2. æˆäº¤é‡å‡çº¿
    for w in [5, 10, 20, 30]:
        df[f'vol_ma{w}'] = df['volume'].rolling(w).mean()

    # 3. å¤æ‚æŒ‡æ ‡ (ä½¿ç”¨ Pandas-TA)
    # ä½¿ç”¨ try-except åŒ…è£¹ï¼Œé˜²æ­¢æŸåªè‚¡ç¥¨æ•°æ®ä¸è¶³å¯¼è‡´æŠ¥é”™
    try:
        # MACD (12, 26, 9)
        macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
        if macd is not None:
            df['dif'] = macd.iloc[:, 0]
            df['macd'] = macd.iloc[:, 1]
            df['dea'] = macd.iloc[:, 2]
    except: pass

    try:
        # KDJ (9, 3, 3)
        kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
        if kdj is not None:
            df['k'] = kdj.iloc[:, 0]
            df['d'] = kdj.iloc[:, 1]
            df['j'] = kdj.iloc[:, 2]
    except: pass

    try:
        # RSI (6, 12, 24)
        df['rsi6'] = df.ta.rsi(close='close', length=6)
        df['rsi12'] = df.ta.rsi(close='close', length=12)
        df['rsi24'] = df.ta.rsi(close='close', length=24)
    except: pass

    try:
        # BOLL (20, 2)
        boll = df.ta.bbands(close='close', length=20, std=2)
        if boll is not None:
            df['boll_lb'] = boll.iloc[:, 0] # Lower
            # Middle (MA20) å·²æœ‰ï¼Œä¸å†å­˜å‚¨
            df['boll_up'] = boll.iloc[:, 2] # Upper
    except: pass
    
    try:
        # CCI (14) & ATR (14)
        df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
        df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
    except: pass

    return clean_indicators(df)

def main():
    print("ğŸš€ å¼€å§‹å…¨é‡åˆå¹¶ä¸å‘¨æœŸç”Ÿæˆ...")
    
    # 1. åŠ è½½å†å²å½’æ¡£ (Cold Data)
    df_history = pd.DataFrame()
    if os.path.exists(ARCHIVE_PATH):
        print(f"ğŸ§Š åŠ è½½å†å²å½’æ¡£: {ARCHIVE_PATH} ...")
        try:
            df_history = pd.read_parquet(ARCHIVE_PATH)
            df_history['date'] = pd.to_datetime(df_history['date'])
        except Exception as e:
            print(f"âš ï¸ å†å²å½’æ¡£åŠ è½½å¤±è´¥: {e}")

    # 2. åŠ è½½ä»Šæ—¥ä¸‹è½½ (New Data from Baostock)
    print("ğŸ”¥ åŠ è½½ä»Šæ—¥å¢é‡æ•°æ®...")
    k_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    f_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    f_map = {os.path.basename(f): f for f in f_files}
    
    new_dfs = []
    for k_path in tqdm(k_files, desc="Reading New"):
        try:
            df = pd.read_parquet(k_path)
            filename = os.path.basename(k_path)
            code = filename.replace('.parquet', '')
            
            # åˆå¹¶èµ„é‡‘æµ
            if filename in f_map:
                try:
                    df_f = pd.read_parquet(f_map[filename])
                    if not df_f.empty:
                        df['date'] = pd.to_datetime(df['date'])
                        df_f['date'] = pd.to_datetime(df_f['date'])
                        df = pd.merge(df, df_f, on=['date', 'code'], how='left')
                except: pass
            
            new_dfs.append(df)
        except: pass
    
    if new_dfs:
        df_new = pd.concat(new_dfs, ignore_index=True)
        df_new['date'] = pd.to_datetime(df_new['date'])
    else:
        df_new = pd.DataFrame()

    # 3. æ‹¼æ¥å…¨é‡ (Memory Merge)
    if df_history.empty and df_new.empty:
        print("âŒ æ— ä»»ä½•æ•°æ®å¯å¤„ç†")
        return

    print("ğŸ”„ åˆå¹¶å…¨é‡æ•°æ® (History + New)...")
    df_total = pd.concat([df_history, df_new])
    # å»é‡ï¼šæŒ‰ä»£ç å’Œæ—¥æœŸå»é‡ï¼Œä¿ç•™æœ€æ–°çš„ï¼ˆæ–°ä¸‹è½½çš„è¦†ç›–å†å²çš„ï¼‰
    df_total.drop_duplicates(subset=['code', 'date'], keep='last', inplace=True)
    df_total.sort_values(['code', 'date'], inplace=True)
    
    # 4. è®¡ç®—å…¨é‡æŒ‡æ ‡ (MAs, MACD, etc.)
    print("ğŸ§® è®¡ç®—ä¸ªè‚¡æ—¥çº¿æŒ‡æ ‡...")
    # ä½¿ç”¨ groupby().apply() å¯¹æ¯åªè‚¡ç¥¨å•ç‹¬è®¡ç®—æŒ‡æ ‡
    # æ³¨æ„ï¼šè¿™æ­¥æ¯”è¾ƒè€—æ—¶ï¼Œä½†åœ¨ GitHub Actions 7GB å†…å­˜ä¸‹é€šå¸¸å¯ä»¥è·‘é€š
    df_total = df_total.groupby('code', group_keys=False).apply(calculate_all_indicators)
    
    # 5. åˆ‡åˆ†ä¸ä¿å­˜ä¸ªè‚¡æ•°æ®
    
    # A. ä¿å­˜ 2025 çƒ­æ•°æ® (Stock Hot Data)
    # åªä¿å­˜ 2025-01-01 åŠä»¥åçš„æ•°æ®ï¼Œè¦†ç›–ä¸Šä¼  OSS
    print("ğŸ’¾ ä¿å­˜ 2025 çƒ­æ•°æ®...")
    df_2025 = df_total[df_total['date'] >= '2025-01-01'].copy()
    df_2025['date'] = df_2025['date'].dt.strftime('%Y-%m-%d')
    df_2025.to_parquet(f"{DIRS['daily']}/stock_2025.parquet", index=False, compression='zstd')
    
    # B. ä¿å­˜ å†å²å½’æ¡£ (ä»…å½“æœ¬åœ°æ²¡æœ‰å½’æ¡£æ–‡ä»¶æ—¶ç”Ÿæˆï¼Œç”¨äºé¦–æ¬¡åˆå§‹åŒ–ä¸Šä¼  Release)
    if not os.path.exists(ARCHIVE_PATH):
        print(f"ğŸ’¾ ç”Ÿæˆå†å²å½’æ¡£è¡¥ä¸ (2005-2024): {ARCHIVE_FILENAME}")
        df_hist_save = df_total[df_total['date'] < '2025-01-01'].copy()
        if not df_hist_save.empty:
            df_hist_save['date'] = df_hist_save['date'].dt.strftime('%Y-%m-%d')
            df_hist_save.to_parquet(f"{DIRS['daily']}/{ARCHIVE_FILENAME}", index=False, compression='zstd')

    # C. ç”Ÿæˆå‘¨çº¿/æœˆçº¿ (Resample)
    print("ğŸ“… ç”Ÿæˆå‘¨çº¿/æœˆçº¿æ•°æ® (å…¨é‡è¦†ç›–)...")
    
    # å®šä¹‰èšåˆè§„åˆ™
    agg_dict = {
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
        'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last',
        'net_flow_amount': 'sum', 'main_net_flow': 'sum'
    }
    # è¿‡æ»¤å‡ºå­˜åœ¨çš„åˆ—
    valid_agg = {k: v for k, v in agg_dict.items() if k in df_total.columns}
    
    # å‘¨çº¿
    df_weekly = df_total.set_index('date').groupby('code').resample('W-FRI').agg(valid_agg).reset_index()
    df_weekly.dropna(subset=['close'], inplace=True)
    df_weekly = df_weekly.groupby('code', group_keys=False).apply(calculate_all_indicators) # è®¡ç®—å‘¨çº¿æŒ‡æ ‡
    df_weekly['date'] = df_weekly['date'].dt.strftime('%Y-%m-%d')
    df_weekly.to_parquet(f"{DIRS['weekly']}/stock_weekly.parquet", index=False, compression='zstd')
    
    # æœˆçº¿
    df_monthly = df_total.set_index('date').groupby('code').resample('ME').agg(valid_agg).reset_index()
    df_monthly.dropna(subset=['close'], inplace=True)
    df_monthly = df_monthly.groupby('code', group_keys=False).apply(calculate_all_indicators) # è®¡ç®—æœˆçº¿æŒ‡æ ‡
    df_monthly['date'] = df_monthly['date'].dt.strftime('%Y-%m-%d')
    df_monthly.to_parquet(f"{DIRS['monthly']}/stock_monthly.parquet", index=False, compression='zstd')

    # ==========================================
    # 6. æ¿å—èµ„é‡‘æµèšåˆè®¡ç®—
    # ==========================================
    print("ğŸ’° æ­£åœ¨è®¡ç®—æ¿å—èµ„é‡‘æµå‘ (åŸºäºä¸ªè‚¡èšåˆ)...")
    
    sector_kline_path = f"{OUTPUT_BASE}/sector_full.parquet"
    relation_path = f"{OUTPUT_BASE}/sector_constituents.parquet"
    
    if os.path.exists(sector_kline_path) and os.path.exists(relation_path):
        try:
            # è¿æ¥ DuckDB
            con = duckdb.connect()
            
            # æ³¨å†Œå†…å­˜ä¸­çš„ä¸ªè‚¡è¡¨ (åªå–éœ€è¦çš„åˆ—ä»¥çœå†…å­˜)
            con.register('stock_data', df_total[['date', 'code', 'net_flow_amount', 'main_net_flow']])
            
            # è¯»å–ç£ç›˜ä¸Šçš„æ¿å—Kçº¿å’Œå…³ç³»è¡¨
            con.execute(f"CREATE TABLE sector_kline AS SELECT * FROM read_parquet('{sector_kline_path}')")
            con.execute(f"CREATE TABLE relations AS SELECT * FROM read_parquet('{relation_path}')")
            
            # æ‰§è¡ŒèšåˆæŸ¥è¯¢
            # é€»è¾‘ï¼šæ¿å—Kçº¿ Left Join (å…³ç³»è¡¨ Join ä¸ªè‚¡è¡¨ Group By æ¿å—,æ—¥æœŸ)
            print("   -> æ‰§è¡Œ DuckDB SQL èšåˆ...")
            sql = """
            WITH sector_flows AS (
                SELECT 
                    r.sector_code,
                    s.date,
                    SUM(s.net_flow_amount) as net_flow_amount,
                    SUM(s.main_net_flow) as main_net_flow
                FROM stock_data s
                JOIN relations r ON s.code = r.stock_code
                GROUP BY r.sector_code, s.date
            )
            SELECT 
                k.*,
                -- ä½¿ç”¨ COALESCE å¡«å……ç©ºå€¼ä¸º 0
                COALESCE(f.net_flow_amount, 0) as net_flow_amount,
                COALESCE(f.main_net_flow, 0) as main_net_flow
            FROM sector_kline k
            LEFT JOIN sector_flows f ON k.code = f.sector_code AND k.date = f.date
            ORDER BY k.code, k.date
            """
            
            df_sector_final = con.execute(sql).fetchdf()
            
            # è¦†ç›–ä¿å­˜
            print(f"ğŸ’¾ æ›´æ–°æ¿å—æ–‡ä»¶ (å«èµ„é‡‘æµ): {sector_kline_path}")
            df_sector_final.to_parquet(sector_kline_path, index=False, compression='zstd')
            con.close()
            
        except Exception as e:
            print(f"âŒ æ¿å—èµ„é‡‘æµè®¡ç®—å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ è·³è¿‡æ¿å—è®¡ç®— (æ–‡ä»¶ç¼ºå¤±)")

    print("âœ… æ‰€æœ‰æ•°æ®å¤„ç†å®Œæ¯•ï¼")

if __name__ == "__main__":
    main()
