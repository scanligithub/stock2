# scripts/merge_data.py
import pandas as pd
import numpy as np
import pandas_ta as ta
import os
import glob
import datetime
from tqdm import tqdm  # ã€å…³é”®ä¿®å¤ã€‘è¡¥ä¸Šäº†è¿™ä¸ªå¯¼å…¥

# === è·¯å¾„é…ç½® ===
CACHE_DIR = "cache_data"          # å†å²æ•°æ®(Release+Cache)å­˜æ”¾åœ°
KLINE_DIR = "downloaded_kline"    # ä»Šæ—¥Kçº¿å¢é‡
FLOW_DIR = "downloaded_fundflow"  # ä»Šæ—¥èµ„é‡‘æµå¢é‡

# è¾“å‡ºç›®å½•
OUTPUT_ENGINE = "final_output/engine"
OUTPUT_DAILY = f"{OUTPUT_ENGINE}/stock_daily"

os.makedirs(OUTPUT_DAILY, exist_ok=True)

# === æŒ‡æ ‡è®¡ç®—å‡½æ•° ===
def calculate_indicators(df):
    """
    è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (MA, VolMA, MACD, KDJ, RSI, BOLL, CCI, ATR)
    df å¿…é¡»æ˜¯å•åªè‚¡ç¥¨ä¸”æŒ‰æ—¥æœŸæ’åº
    """
    # 1. ä»·æ ¼å‡çº¿ (MA)
    for w in [5, 10, 20, 60, 120, 250]:
        df[f'ma{w}'] = df['close'].rolling(window=w).mean()
    
    # 2. æˆäº¤é‡å‡çº¿ (Vol MA)
    for w in [5, 10, 20, 30]:
        df[f'vol_ma{w}'] = df['volume'].rolling(window=w).mean()

    # 3. MACD (12, 26, 9)
    try:
        macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
        if macd is not None:
            df['dif'] = macd.iloc[:, 0]
            df['macd'] = macd.iloc[:, 1]
            df['dea'] = macd.iloc[:, 2]
    except: pass

    # 4. KDJ (9, 3, 3)
    try:
        kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
        if kdj is not None:
            df['k'] = kdj.iloc[:, 0]
            df['d'] = kdj.iloc[:, 1]
            df['j'] = kdj.iloc[:, 2]
    except: pass

    # 5. RSI (6, 12, 24)
    try:
        df['rsi6'] = df.ta.rsi(close='close', length=6)
        df['rsi12'] = df.ta.rsi(close='close', length=12)
        df['rsi24'] = df.ta.rsi(close='close', length=24)
    except: pass

    # 6. BOLL (20, 2)
    try:
        boll = df.ta.bbands(close='close', length=20, std=2)
        if boll is not None:
            df['boll_lb'] = boll.iloc[:, 0]
            df['boll_up'] = boll.iloc[:, 2]
    except: pass

    # 7. å…¶ä»–
    try:
        df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
        df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
    except: pass

    return df

def process_resample(df_daily, freq, filename):
    """ç”Ÿæˆå‘¨çº¿/æœˆçº¿æ•°æ®"""
    print(f"   -> æ­£åœ¨ç”Ÿæˆ {freq} å‘¨æœŸæ•°æ® ({filename})...")
    
    # èšåˆè§„åˆ™
    agg_rules = {
        'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
        'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
        'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
    }
    # èµ„é‡‘æµç´¯åŠ 
    for c in ['net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow']:
        if c in df_daily.columns: agg_rules[c] = 'sum'

    # é‡é‡‡æ ·
    # 'date' å·²ç»æ˜¯ datetimeIndex
    df_res = df_daily.set_index('date').groupby('code').resample(freq).agg(agg_rules)
    
    # æ¸…æ´—æ— æ•ˆè¡Œ
    df_res = df_res.dropna(subset=['close']).reset_index()
    
    # æ’åº
    df_res.sort_values(['code', 'date'], inplace=True)
    
    # è®¡ç®—å‘¨/æœˆçº¿æŒ‡æ ‡ (ç®€å•ç‰ˆï¼Œåªç®—å‡çº¿)
    grouped = df_res.groupby('code')['close']
    df_res['ma5'] = grouped.rolling(5).mean().reset_index(0, drop=True)
    df_res['ma10'] = grouped.rolling(10).mean().reset_index(0, drop=True)
    df_res['ma20'] = grouped.rolling(20).mean().reset_index(0, drop=True)
    
    # æ ¼å¼åŒ–ä¸å‹ç¼©
    df_res['date'] = df_res['date'].dt.strftime('%Y-%m-%d')
    
    float_cols = df_res.select_dtypes(include=['float64']).columns
    for c in float_cols:
        df_res[c] = df_res[c].round(3).astype('float32')

    out_path = f"{OUTPUT_ENGINE}/{filename}"
    df_res.to_parquet(out_path, index=False, compression='zstd')
    print(f"      âœ… å·²ä¿å­˜: {len(df_res)} è¡Œ")

def main():
    print("ğŸš€ å¼€å§‹å…¨é‡åˆå¹¶ä¸å‘¨æœŸç”Ÿæˆ (å†…å­˜ä¼˜åŒ–ç‰ˆ)...")
    
    current_year = datetime.datetime.now().year
    
    # 1. åŠ è½½å†å²æ•°æ® (Release + Cache)
    history_files = glob.glob(f"{CACHE_DIR}/*.parquet")
    if history_files:
        print(f"ğŸ“¦ åŠ è½½å†å²æ–‡ä»¶: {len(history_files)} ä¸ª")
        df_history = pd.concat([pd.read_parquet(f) for f in history_files], ignore_index=True)
    else:
        print("âš ï¸ æœªæ‰¾åˆ°å†å²æ•°æ®ï¼Œå°†è¿›è¡Œå…¨é‡åˆå§‹åŒ–")
        df_history = pd.DataFrame()

    # 2. åŠ è½½ä»Šæ—¥å¢é‡
    k_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    f_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    print(f"ğŸ”¥ åŠ è½½ä»Šæ—¥å¢é‡: {len(k_files)} ä¸ª Kçº¿æ–‡ä»¶")
    
    f_map = {os.path.basename(f): f for f in f_files}
    dfs_new = []
    
    # ã€ä¿®å¤ç‚¹ã€‘è¿™é‡Œä½¿ç”¨äº† tqdmï¼Œä¹‹å‰æŠ¥é”™å°±æ˜¯å› ä¸ºæ²¡ import
    for k_f in tqdm(k_files, desc="Reading New"):
        try:
            df_k = pd.read_parquet(k_f)
            if df_k.empty: continue
            
            # ç»Ÿä¸€è½¬ datetime æ–¹ä¾¿ merge
            df_k['date'] = pd.to_datetime(df_k['date'])
            
            fname = os.path.basename(k_f)
            if fname in f_map:
                df_f = pd.read_parquet(f_map[fname])
                if not df_f.empty:
                    df_f['date'] = pd.to_datetime(df_f['date'])
                    df_k = pd.merge(df_k, df_f, on=['date', 'code'], how='left')
            
            dfs_new.append(df_k)
        except: pass
        
    if dfs_new:
        df_new = pd.concat(dfs_new, ignore_index=True)
    else:
        df_new = pd.DataFrame()

    # 3. åˆå¹¶å…¨é‡
    if df_history.empty and df_new.empty:
        print("âŒ æ— æ•°æ®å¤„ç†")
        return

    # ç»Ÿä¸€æ ¼å¼
    if not df_history.empty: df_history['date'] = pd.to_datetime(df_history['date'])
    # df_new å·²ç»æ˜¯ datetime

    print("ğŸ”„ åˆå¹¶å†å²ä¸æ–°å¢...")
    df_total = pd.concat([df_history, df_new], ignore_index=True)
    
    # å»é‡ (é˜²æ­¢é‡å¤è¿è¡Œ)
    df_total.drop_duplicates(subset=['code', 'date'], keep='last', inplace=True)
    df_total.sort_values(['code', 'date'], inplace=True)
    
    # 4. è®¡ç®—å…¨é‡æŒ‡æ ‡
    print("ğŸ§® è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (è€—æ—¶æ“ä½œ)...")
    # ä½¿ç”¨ groupby apply è¿›è¡Œå¹¶è¡Œè®¡ç®—
    df_total = df_total.groupby('code', group_keys=False).apply(calculate_indicators)
    
    # 5. ç”Ÿæˆå‘¨çº¿/æœˆçº¿
    # (æ­¤æ—¶ date è¿˜æ˜¯ datetime ç±»å‹ï¼Œæ­£å¥½ç”¨äº resample)
    print("ğŸ“… ç”Ÿæˆå¤šå‘¨æœŸæ•°æ®...")
    process_resample(df_total, 'W-FRI', 'stock_weekly.parquet')
    process_resample(df_total, 'ME', 'stock_monthly.parquet')

    # 6. æ•°æ®ç±»å‹å‹ç¼© (å‡†å¤‡ä¿å­˜)
    print("ğŸ’¾ æ•°æ®ç±»å‹ä¼˜åŒ–...")
    float_cols = df_total.select_dtypes(include=['float64']).columns
    for c in float_cols:
        df_total[c] = df_total[c].round(3).astype('float32')
        
    # è¿˜åŸæ—¥æœŸä¸ºå­—ç¬¦ä¸²
    df_total['date'] = df_total['date'].dt.strftime('%Y-%m-%d')

    # 7. åˆ‡åˆ†è¾“å‡º
    # A. ä¿å­˜ Cache (ä¾›æ˜å¤©ç”¨ï¼Œä»…ä¿ç•™å½“å¹´çš„çƒ­æ•°æ®)
    df_hot = df_total[df_total['date'] >= f"{current_year}-01-01"].copy()
    cache_path = f"{CACHE_DIR}/stock_current_year.parquet"
    print(f"ğŸ“¦ æ›´æ–° Cache æ–‡ä»¶: {cache_path} ({len(df_hot)} è¡Œ)")
    df_hot.to_parquet(cache_path, index=False, compression='zstd')

    # B. ä¿å­˜ OSS (ä»…ä¿å­˜å½“å¹´çš„æ–‡ä»¶åˆ° stock_daily ç›®å½•)
    oss_path = f"{OUTPUT_DAILY}/stock_{current_year}.parquet"
    print(f"â˜ï¸ ç”Ÿæˆ OSS æ–‡ä»¶: {oss_path}")
    df_hot.to_parquet(oss_path, index=False, compression='zstd')

    print("âœ… å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
