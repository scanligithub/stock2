# scripts/merge_data.py
import pandas as pd
import glob
import os
from tqdm import tqdm
import pandas_ta as ta  # å¼•å…¥æŠ€æœ¯åˆ†æåº“
import numpy as np
import duckdb
import shutil

# è¾“å…¥ç›®å½•
KLINE_DIR = "downloaded_kline" 
FLOW_DIR = "downloaded_fundflow"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "final_output/engine"
TEMP_DIR = "temp_chunks" # ä¸´æ—¶åˆ†æ‰¹ç›®å½•

os.makedirs(OUTPUT_DIR, exist_ok=True)
if os.path.exists(TEMP_DIR):
    shutil.rmtree(TEMP_DIR)
os.makedirs(TEMP_DIR, exist_ok=True)

BATCH_SIZE = 500

def clean_indicators(df):
    """
    å¼ºåˆ¶æ¸…æ´—æŒ‡æ ‡åˆ—ï¼Œé˜²æ­¢ 'Object' ç±»å‹æˆ–æ··å…¥ Timestamp
    """
    # å®šä¹‰éœ€è¦æ¸…æ´—çš„åˆ—å (æŒ‡æ ‡ + èµ„é‡‘æµ + å› å­)
    target_cols = [
        # MACD
        'dif', 'dea', 'macd', 
        # KDJ
        'k', 'd', 'j',
        # RSI
        'rsi6', 'rsi12', 'rsi24',
        # BOLL
        'boll_up', 'boll_lb',
        # Other
        'cci', 'atr',
        # Funds
        'net_flow_amount', 'main_net_flow', 'super_large_net_flow', 
        'large_net_flow', 'medium_small_net_flow',
        # Factors
        'peTTM', 'pbMRQ', 'adjustFactor', 'mkt_cap'
    ]
    
    # åŠ ä¸Šæ‰€æœ‰å‡çº¿ (å«ä»·æ ¼å’Œé‡èƒ½)
    for w in [5, 10, 20, 60, 120, 250]: target_cols.append(f'ma{w}')
    for w in [5, 10, 20, 30]: target_cols.append(f'vol_ma{w}')

    # æ‰§è¡Œè½¬æ¢
    for col in target_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')
    
    return df

def process_single_stock(k_path, f_map):
    """
    å•åªè‚¡ç¥¨å¤„ç†é€»è¾‘ï¼šåˆå¹¶èµ„é‡‘æµ -> è®¡ç®—æŒ‡æ ‡
    """
    try:
        # 1. è¯»å– K çº¿
        df = pd.read_parquet(k_path)
        if df.empty: return None
        
        filename = os.path.basename(k_path)
        code = filename.replace('.parquet', '')
        
        # 2. åˆå¹¶èµ„é‡‘æµ
        if filename in f_map:
            f_path = f_map[filename]
            try:
                df_f = pd.read_parquet(f_path)
                if not df_f.empty:
                    # ç»Ÿä¸€è½¬æ¢ä¸º datetime ç”¨äºåˆå¹¶
                    df['date'] = pd.to_datetime(df['date'])
                    df_f['date'] = pd.to_datetime(df_f['date'])
                    df = pd.merge(df, df_f, on=['date', 'code'], how='left')
            except Exception:
                pass # èµ„é‡‘æµå‡ºé”™ä¸å½±å“ K çº¿
        
        # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº (è®¡ç®—æŒ‡æ ‡å¿…é¡»)
        if df['date'].dtype != 'datetime64[ns]':
            df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)

        # 3. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (Pandas-TA)
        
        # A. ä»·æ ¼å‡çº¿ (ä½¿ç”¨ Pandas åŸç”Ÿ Rolling æé€Ÿ)
        for w in [5, 10, 20, 60, 120, 250]:
            df[f'ma{w}'] = df['close'].rolling(w).mean()
        
        # B. æˆäº¤é‡å‡çº¿
        for w in [5, 10, 20, 30]:
            df[f'vol_ma{w}'] = df['volume'].rolling(w).mean()

        # C. å¤æ‚æŒ‡æ ‡
        # MACD (12, 26, 9)
        try:
            macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
            if macd is not None:
                df['dif'] = macd.iloc[:, 0]
                df['macd'] = macd.iloc[:, 1]
                df['dea'] = macd.iloc[:, 2]
        except: pass

        # KDJ (9, 3, 3)
        try:
            kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
            if kdj is not None:
                df['k'] = kdj.iloc[:, 0]
                df['d'] = kdj.iloc[:, 1]
                df['j'] = kdj.iloc[:, 2]
        except: pass

        # RSI (6, 12, 24)
        try:
            df['rsi6'] = df.ta.rsi(close='close', length=6)
            df['rsi12'] = df.ta.rsi(close='close', length=12)
            df['rsi24'] = df.ta.rsi(close='close', length=24)
        except: pass

        # BOLL (20, 2)
        try:
            boll = df.ta.bbands(close='close', length=20, std=2)
            if boll is not None:
                df['boll_lb'] = boll.iloc[:, 0]
                df['boll_up'] = boll.iloc[:, 2]
        except: pass
        
        # CCI & ATR (14)
        try:
            df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
            df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
        except: pass

        # 4. å¼ºåˆ¶ç±»å‹æ¸…æ´—
        df = clean_indicators(df)

        return df

    except Exception as e:
        print(f"Err processing {k_path}: {e}")
        return None

def main():
    print("ğŸš€ å¼€å§‹å®½è¡¨åˆå¹¶ä¸æŒ‡æ ‡è®¡ç®— (å†…å­˜å®‰å…¨ç‰ˆ)...")
    
    k_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    f_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    print(f"å¾…å¤„ç†è‚¡ç¥¨: {len(k_files)}")
    
    f_map = {os.path.basename(f): f for f in f_files}
    
    # --- ç¬¬ä¸€é˜¶æ®µï¼šåˆ†æ‰¹è®¡ç®—å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ ---
    batch_buffer = []
    chunk_index = 0
    
    for i, k_path in enumerate(tqdm(k_files, desc="Processing Batches")):
        df = process_single_stock(k_path, f_map)
        if df is not None:
            batch_buffer.append(df)
        
        # å‡‘å¤Ÿä¸€æ‰¹å†™å…¥ä¸€æ¬¡
        if len(batch_buffer) >= BATCH_SIZE or (i == len(k_files) - 1 and batch_buffer):
            chunk_df = pd.concat(batch_buffer, ignore_index=True)
            
            # è¿˜åŸæ—¥æœŸæ ¼å¼
            chunk_df['date'] = chunk_df['date'].dt.strftime('%Y-%m-%d')
            
            # å†™å…¥
            temp_path = f"{TEMP_DIR}/chunk_{chunk_index}.parquet"
            chunk_df.to_parquet(temp_path, index=False, compression='zstd', engine='pyarrow')
            
            batch_buffer = []
            chunk_index += 1
            
    print(f"\nâœ… æ‰¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {chunk_index} ä¸ªä¸´æ—¶å—ã€‚å¼€å§‹æœ€ç»ˆåˆå¹¶...")

    # --- ç¬¬äºŒé˜¶æ®µï¼šDuckDB åˆå¹¶ ---
    final_output = f"{OUTPUT_DIR}/stock_full.parquet"
    
    try:
        con = duckdb.connect()
        print("ğŸ¦† DuckDB Merging...")
        
        # ä½¿ç”¨ COPY å‘½ä»¤å°†å¤šä¸ªå°æ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªå¤§æ–‡ä»¶ï¼Œä¸”æŒ‰ code, date æ’åº
        query = f"""
        COPY (
            SELECT * FROM read_parquet('{TEMP_DIR}/*.parquet')
            ORDER BY code, date
        ) TO '{final_output}' (FORMAT 'PARQUET', COMPRESSION 'ZSTD', ROW_GROUP_SIZE 100000);
        """
        con.execute(query)
        con.close()
        
        print(f"âœ… æœ€ç»ˆæ–‡ä»¶ç”Ÿæˆå®Œæ¯•: {final_output}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(TEMP_DIR)
        
    except Exception as e:
        print(f"âŒ DuckDB Merge Failed: {e}")
        # å¦‚æœ DuckDB å¤±è´¥ï¼Œè„šæœ¬æŠ¥é”™é€€å‡º
        exit(1)

if __name__ == "__main__":
    main()
