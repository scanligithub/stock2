# scripts/merge_data.py
import pandas as pd
import glob
import os
import gc  # å¼•å…¥åƒåœ¾å›æ”¶
import pandas_ta as ta
import numpy as np
import datetime

# è·¯å¾„é…ç½®
CACHE_DIR = "cache_data" 
TODAY_DIR = "downloaded_kline" # æ³¨æ„ï¼šè¿™é‡Œè¦å¯¹åº” artifact ä¸‹è½½åçš„ç›®å½•å
FUND_DIR = "downloaded_fundflow"
OUTPUT_DIR = "final_output/engine"

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(f"{OUTPUT_DIR}/stock_daily", exist_ok=True)

def optimize_types(df):
    """å°† float64 é™çº§ä¸º float32 ä»¥èŠ‚çœå†…å­˜"""
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32')
    return df

def calculate_indicators(df):
    """è®¡ç®—å•åªè‚¡ç¥¨çš„æŒ‡æ ‡ (è¾“å…¥å·²æ’åº)"""
    # 1. ä»·æ ¼å‡çº¿
    for w in [5, 10, 20, 60, 120, 250]:
        df[f'ma{w}'] = df['close'].rolling(w).mean()
    
    # 2. é‡å‡çº¿
    for w in [5, 10, 20, 30]:
        df[f'vol_ma{w}'] = df['volume'].rolling(w).mean()

    # 3. å¤æ‚æŒ‡æ ‡ (ä½¿ç”¨ pandas_ta)
    try:
        # MACD (12,26,9)
        macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
        if macd is not None:
            df['dif'] = macd.iloc[:, 0]
            df['dea'] = macd.iloc[:, 2]
            df['macd'] = macd.iloc[:, 1]

        # KDJ (9,3,3)
        kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
        if kdj is not None:
            df['k'] = kdj.iloc[:, 0]
            df['d'] = kdj.iloc[:, 1]
            df['j'] = kdj.iloc[:, 2]

        # RSI (6,12,24)
        df['rsi6'] = df.ta.rsi(close='close', length=6)
        df['rsi12'] = df.ta.rsi(close='close', length=12)
        df['rsi24'] = df.ta.rsi(close='close', length=24)

        # BOLL (20,2)
        boll = df.ta.bbands(close='close', length=20, std=2)
        if boll is not None:
            df['boll_up'] = boll.iloc[:, 2]
            df['boll_lb'] = boll.iloc[:, 0]

        # CCI & ATR
        df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
        df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)

    except Exception:
        pass

    return df

def main():
    print("ğŸš€ å¼€å§‹å…¨é‡åˆå¹¶ä¸å‘¨æœŸç”Ÿæˆ (å†…å­˜ä¼˜åŒ–ç‰ˆ)...")
    
    # === 1. åŠ è½½å¹¶åˆå¹¶èµ„é‡‘æµ (å¦‚æœæœ‰) ===
    # ä¸ºäº†çœå†…å­˜ï¼Œæˆ‘ä»¬å»ºç«‹ä¸€ä¸ª {code_date: flow_data} çš„å­—å…¸æˆ–è€…å…ˆä¸å¤„ç†
    # é‰´äºèµ„é‡‘æµæ–‡ä»¶è¾ƒå¤šï¼Œå»ºè®®å…ˆå¤„ç† K çº¿ï¼Œæœ€åå† Join èµ„é‡‘æµï¼Œæˆ–è€…åˆ†å—å¤„ç†
    # è¿™é‡Œä¸ºäº†é€»è¾‘ç®€å•ï¼Œæš‚ä¸æ”¹å˜æ•´ä½“æµç¨‹ï¼Œä½†åŠ å¼ºå†…å­˜å›æ”¶
    
    # === 2. åŠ è½½ K çº¿æ•°æ® ===
    history_files = glob.glob(f"{CACHE_DIR}/*.parquet")
    new_files = glob.glob(f"{TODAY_DIR}/*.parquet")
    
    # --- åˆ†æ‰¹è¯»å– History ---
    df_history = pd.DataFrame()
    if history_files:
        print(f"ğŸ“¦ åŠ è½½å†å²æ–‡ä»¶: {len(history_files)} ä¸ª")
        # é€ä¸ªè¯»å–å¹¶ç«‹å³è½¬æ¢ç±»å‹
        dfs = []
        for f in history_files:
            _df = pd.read_parquet(f)
            _df = optimize_types(_df)
            dfs.append(_df)
        df_history = pd.concat(dfs, ignore_index=True)
        del dfs # ç«‹å³é‡Šæ”¾åˆ—è¡¨
        gc.collect() # å¼ºåˆ¶å›æ”¶

    # --- åˆ†æ‰¹è¯»å– New Data ---
    df_new = pd.DataFrame()
    if new_files:
        print(f"ğŸ”¥ åŠ è½½ä»Šæ—¥å¢é‡: {len(new_files)} ä¸ª")
        dfs = []
        for f in tqdm(new_files, desc="Reading New"):
            _df = pd.read_parquet(f)
            _df = optimize_types(_df) # ç«‹å³ç˜¦èº«
            dfs.append(_df)
        df_new = pd.concat(dfs, ignore_index=True)
        del dfs
        gc.collect()

    # === 3. åˆå¹¶å…¨é‡ ===
    if df_history.empty and df_new.empty:
        print("âŒ æ— æ•°æ®")
        return

    print("ğŸ”„ æ‰§è¡Œå…¨é‡æ‹¼æ¥...")
    # ç»Ÿä¸€æ—¥æœŸæ ¼å¼
    if not df_history.empty:
        df_history['date'] = pd.to_datetime(df_history['date'])
    if not df_new.empty:
        df_new['date'] = pd.to_datetime(df_new['date'])

    df_total = pd.concat([df_history, df_new])
    
    # é‡Šæ”¾æ—§å¯¹è±¡
    del df_history, df_new
    gc.collect()
    print("ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ")

    # å»é‡æ’åº
    print("âš¡ æ’åºä¸å»é‡...")
    df_total.drop_duplicates(subset=['code', 'date'], keep='last', inplace=True)
    df_total.sort_values(['code', 'date'], inplace=True)

    # === 4. è®¡ç®—æŒ‡æ ‡ ===
    print("ğŸ§® è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (åˆ†ç»„è¿ç®—)...")
    # ä½¿ç”¨ groupby().apply() ä¼šäº§ç”Ÿå¤§é‡ä¸´æ—¶å†…å­˜ï¼Œè¿™é‡Œä¼˜åŒ–ä¸€ä¸‹ï¼š
    # ç›´æ¥åœ¨åŸ DataFrame ä¸Šæ“ä½œå¯èƒ½æ›´çœå†…å­˜ï¼Œä½†è¿™éœ€è¦æå…¶å¤æ‚çš„å‘é‡åŒ–å†™æ³•
    # æˆ‘ä»¬ä¿æŒ apply ä½†ç¡®ä¿ df_total å·²ç»æ˜¯ float32
    df_total = df_total.groupby('code', group_keys=False).apply(calculate_indicators)
    
    # å†æ¬¡ä¼˜åŒ–ç±»å‹ (æŒ‡æ ‡è®¡ç®—å¯èƒ½å¼•å…¥äº† float64)
    df_total = optimize_types(df_total)
    
    # === 5. Join èµ„é‡‘æµ (å¦‚æœæœ‰) ===
    # (å¦‚æœèµ„é‡‘æµæ•°æ®é‡å¤§ï¼Œå»ºè®®æ”¾åœ¨è®¡ç®—æŒ‡æ ‡ä¹‹å‰ Joinï¼Œæˆ–è€…å•ç‹¬å¤„ç†)
    # è¿™é‡Œå‡è®¾èµ„é‡‘æµå·²åŒ…å«åœ¨ K çº¿ä¸‹è½½é€»è¾‘ä¸­ï¼Œæˆ–è€…åœ¨æ­¤å¤„ç®€å•å¤„ç†
    
    # === 6. ç”Ÿæˆå‘¨æœŸæ•°æ® (å‘¨/æœˆ) ===
    # å®šä¹‰èšåˆè§„åˆ™
    agg_rules = {
        'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
        'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
        'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
    }
    # åŠ¨æ€æ·»åŠ å­˜åœ¨çš„æŒ‡æ ‡åˆ—ï¼Œé€šå¸¸åªå¯¹ OHLCV èšåˆï¼ŒæŒ‡æ ‡é‡æ–°ç®—
    
    def resample_and_save(df_src, freq, name):
        print(f"ğŸ“… ç”Ÿæˆ {name} æ•°æ®...")
        # ä»…å¯¹å¿…è¦åˆ—è¿›è¡Œé‡é‡‡æ ·ï¼Œé¿å…ä¸å¿…è¦çš„è®¡ç®—
        valid_cols = [c for c in agg_rules.keys() if c in df_src.columns]
        current_rules = {k: agg_rules[k] for k in valid_cols}
        
        df_res = df_src.set_index('date').groupby('code').resample(freq).agg(current_rules)
        df_res = df_res.dropna(subset=['close']).reset_index()
        df_res.sort_values(['code', 'date'], inplace=True)
        
        # é‡ç®—å‘¨/æœˆçº¿æŒ‡æ ‡
        df_res = df_res.groupby('code', group_keys=False).apply(calculate_indicators)
        df_res = optimize_types(df_res)
        
        # ä¿å­˜
        df_res['date'] = df_res['date'].dt.strftime('%Y-%m-%d')
        out = f"{OUTPUT_DIR}/{name}.parquet"
        print(f"ğŸ’¾ ä¿å­˜: {out}")
        df_res.to_parquet(out, index=False, compression='zstd')
        
        del df_res
        gc.collect()

    # æ‰§è¡Œé‡é‡‡æ ·
    resample_and_save(df_total, 'W-FRI', 'stock_weekly')
    resample_and_save(df_total, 'ME', 'stock_monthly')

    # === 7. ä¿å­˜æ—¥çº¿æ•°æ® ===
    # A. æ›´æ–° Cache (å…¨å¹´)
    current_year = datetime.datetime.now().year
    df_cache = df_total[df_total['date'].dt.year == current_year].copy()
    df_cache['date'] = df_cache['date'].dt.strftime('%Y-%m-%d')
    
    print(f"ğŸ’¾ ä¿å­˜ Cache: stock_current_year.parquet")
    df_cache.to_parquet(f"{CACHE_DIR}/stock_current_year.parquet", index=False, compression='zstd')
    del df_cache
    gc.collect()

    # B. æ›´æ–° OSS (åŒä¸Šï¼Œä¹Ÿå°±æ˜¯ Cache æ–‡ä»¶)
    # å› ä¸º OSS ä¹Ÿæ˜¯æŒ‰å¹´å­˜çš„ï¼Œç›´æ¥å¤åˆ¶å³å¯ï¼Œæˆ–è€…è¿™é‡Œå†å­˜ä¸€ä»½
    oss_path = f"{OUTPUT_DIR}/stock_daily/stock_{current_year}.parquet"
    print(f"ğŸ’¾ ä¿å­˜ OSS: {oss_path}")
    # è¿™é‡Œå·æ‡’ç›´æ¥è¯»åˆšæ‰å­˜çš„ cache æ–‡ä»¶å¤åˆ¶è¿‡å»ï¼Œæˆ–è€…ç”¨ df_cache (å¦‚æœæ²¡åˆ )
    # ç”±äºåˆšæ‰ del äº†ï¼Œè¿™é‡Œé‡æ–°ç­›é€‰ä¸€ä¸‹æˆ–è€…æ‹·è´æ–‡ä»¶
    # æ—¢ç„¶ df_total è¿˜åœ¨ï¼Œå†åˆ‡ä¸€æ¬¡ä¹Ÿå¾ˆå¿«
    df_oss = df_total[df_total['date'].dt.year == current_year].copy()
    df_oss['date'] = df_oss['date'].dt.strftime('%Y-%m-%d')
    df_oss.to_parquet(oss_path, index=False, compression='zstd')

    print("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
