# scripts/merge_data.py
import duckdb
import pandas as pd
import pandas_ta as ta
import numpy as np
import os
import glob
import datetime
import pyarrow as pa
import pyarrow.parquet as pq
from tqdm import tqdm
import gc

# === è·¯å¾„é…ç½® ===
CACHE_DIR = "cache_data"          
KLINE_DIR = "downloaded_kline"    
FLOW_DIR = "downloaded_fundflow"  

# è¾“å‡ºç›®å½•
OUTPUT_ENGINE = "final_output/engine"
OUTPUT_DAILY = f"{OUTPUT_ENGINE}/stock_daily" 
CACHE_OUTPUT_FILE = f"{CACHE_DIR}/stock_buffer.parquet" 

os.makedirs(OUTPUT_DAILY, exist_ok=True)

# === DuckDB åˆå§‹åŒ– ===
con = duckdb.connect(database=':memory:')
con.execute("SET memory_limit='4GB';") 
con.execute("SET threads=2;")

def get_all_codes():
    """è·å–å…¨é‡è‚¡ç¥¨ä»£ç """
    codes = set()
    history_files = glob.glob(f"{CACHE_DIR}/*.parquet")
    if history_files:
        print(f"ğŸ“¦ æ‰«æå†å²æ–‡ä»¶: {len(history_files)} ä¸ª")
        try:
            files_sql = str(history_files).replace('[', '').replace(']', '')
            df_codes = con.execute(f"SELECT DISTINCT code FROM read_parquet({files_sql})").fetchdf()
            codes.update(df_codes['code'].tolist())
        except Exception as e:
            print(f"âš ï¸ è¯»å–å†å²ä»£ç å¤±è´¥: {e}")

    kline_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    print(f"ğŸ”¥ æ‰«æä»Šæ—¥å¢é‡: {len(kline_files)} ä¸ª")
    for f in kline_files:
        code = os.path.basename(f).replace('.parquet', '')
        codes.add(code)
        
    return sorted(list(codes)), history_files, kline_files

def calculate_indicators(df):
    """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
    # 1. ä»·æ ¼å‡çº¿
    for w in [5, 10, 20, 60, 120, 250]:
        df[f'ma{w}'] = df['close'].rolling(window=w).mean()
    
    # 2. æˆäº¤é‡å‡çº¿
    for w in [5, 10, 20, 30]:
        df[f'vol_ma{w}'] = df['volume'].rolling(window=w).mean()

    # 3. MACD
    try:
        macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
        if macd is not None:
            df['dif'] = macd.iloc[:, 0]
            df['macd'] = macd.iloc[:, 1]
            df['dea'] = macd.iloc[:, 2]
    except: pass

    # 4. KDJ
    try:
        kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
        if kdj is not None:
            df['k'] = kdj.iloc[:, 0]
            df['d'] = kdj.iloc[:, 1]
            df['j'] = kdj.iloc[:, 2]
    except: pass

    # 5. RSI
    try:
        df['rsi6'] = df.ta.rsi(close='close', length=6)
        df['rsi12'] = df.ta.rsi(close='close', length=12)
        df['rsi24'] = df.ta.rsi(close='close', length=24)
    except: pass

    # 6. BOLL
    try:
        boll = df.ta.bbands(close='close', length=20, std=2)
        if boll is not None:
            df['boll_lb'] = boll.iloc[:, 0]
            df['boll_up'] = boll.iloc[:, 2]
    except: pass

    # 7. å…¶ä»–
    try:
        df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
        df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
    except: pass

    return df

def process_resample(writer_w, writer_m, df_daily):
    """æµå¼ç”Ÿæˆå‘¨/æœˆçº¿"""
    if df_daily.empty: return

    agg_rules = {
        'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
        'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
        'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
    }
    # èµ„é‡‘æµç´¯åŠ 
    flow_cols = [c for c in df_daily.columns if 'flow' in c]
    for c in flow_cols: agg_rules[c] = 'sum'

    # å‘¨çº¿
    try:
        df_w = df_daily.set_index('date').resample('W-FRI').agg(agg_rules).dropna(subset=['close']).reset_index()
        if not df_w.empty:
            df_w['code'] = df_daily['code'].iloc[0]
            for w in [5, 10, 20]:
                df_w[f'ma{w}'] = df_w['close'].rolling(w).mean()
            float_cols = df_w.select_dtypes(include=['float64']).columns
            df_w[float_cols] = df_w[float_cols].astype('float32')
            table_w = pa.Table.from_pandas(df_w)
            writer_w.write_table(table_w)
    except: pass

    # æœˆçº¿
    try:
        df_m = df_daily.set_index('date').resample('ME').agg(agg_rules).dropna(subset=['close']).reset_index()
        if not df_m.empty:
            df_m['code'] = df_daily['code'].iloc[0]
            for w in [5, 10, 20]:
                df_m[f'ma{w}'] = df_m['close'].rolling(w).mean()
            float_cols = df_m.select_dtypes(include=['float64']).columns
            df_m[float_cols] = df_m[float_cols].astype('float32')
            table_m = pa.Table.from_pandas(df_m)
            writer_m.write_table(table_m)
    except: pass

def main():
    print("ğŸš€ å¼€å§‹ DuckDB æµå¼åˆå¹¶ä¸è®¡ç®— (Schema é”å®šç‰ˆ)...")
    
    all_codes, history_files, kline_files = get_all_codes()
    if not all_codes:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è‚¡ç¥¨ä»£ç ")
        return
    print(f"âœ… æ€»è®¡éœ€å¤„ç†: {len(all_codes)} åªè‚¡ç¥¨")

    # å»ºç«‹ç´¢å¼•
    kline_map = {os.path.basename(f).replace('.parquet', ''): f for f in kline_files}
    flow_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    flow_map = {os.path.basename(f).replace('.parquet', ''): f for f in flow_files}

    # æ³¨å†Œå†å²è§†å›¾
    has_history = False
    if history_files:
        files_sql = str(history_files).replace('[', '').replace(']', '')
        con.execute(f"CREATE OR REPLACE VIEW history_view AS SELECT * FROM read_parquet({files_sql})")
        has_history = True

    # =========================================================
    # ã€æ ¸å¿ƒä¿®å¤ã€‘æ„é€ æ ‡å‡† Dummy Schema
    # ä¸ä¾èµ– sample_df æ¨æ–­ï¼Œè€Œæ˜¯æ‰‹åŠ¨å®šä¹‰å…¨é‡å­—æ®µæ¨¡æ¿
    # ç¡®ä¿ Date æ˜¯ stringï¼Œæ•°å€¼æ˜¯ float32
    # =========================================================
    print("ğŸ”’ é”å®šæ•°æ® Schema...")
    
    dummy_data = {
        'date': ['2025-01-01'], # å¼ºåˆ¶ String
        'code': ['dummy'],      # å¼ºåˆ¶ String
    }
    # å®šä¹‰æ‰€æœ‰å¯èƒ½å‡ºç°çš„æ•°å€¼åˆ—
    float_cols_def = [
        'open', 'close', 'high', 'low', 'volume', 'amount', 'turn', 'pctChg', 
        'peTTM', 'pbMRQ', 'adjustFactor', 'mkt_cap', 
        'net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow',
        'ma5', 'ma10', 'ma20', 'ma60', 'ma120', 'ma250',
        'vol_ma5', 'vol_ma10', 'vol_ma20', 'vol_ma30',
        'dif', 'dea', 'macd', 'k', 'd', 'j', 'rsi6', 'rsi12', 'rsi24',
        'boll_lb', 'boll_up', 'cci', 'atr'
    ]
    
    for c in float_cols_def:
        dummy_data[c] = [0.0] # åˆå§‹ä¸º float
        
    df_schema_template = pd.DataFrame(dummy_data)
    
    # å¼ºåˆ¶è½¬æ¢ç±»å‹
    df_schema_template[float_cols_def] = df_schema_template[float_cols_def].astype('float32')
    
    # è·å–ç»å¯¹æ­£ç¡®çš„ Schema
    final_schema = pa.Table.from_pandas(df_schema_template).schema
    
    # åˆå§‹åŒ– Writers
    writer_buffer = pq.ParquetWriter(CACHE_OUTPUT_FILE, final_schema, compression='zstd')
    
    current_year = datetime.datetime.now().year
    oss_file = f"{OUTPUT_DAILY}/stock_{current_year}.parquet"
    writer_oss = pq.ParquetWriter(oss_file, final_schema, compression='zstd')
    
    weekly_buffer = []
    monthly_buffer = []

    # 4. ğŸš€ å¾ªç¯å¤„ç†
    print("ğŸŒŠ å¼€å§‹æµå¼å¤„ç†...")
    
    for code in tqdm(all_codes):
        # A. è¯»å–å†å²
        df_hist = pd.DataFrame()
        if has_history:
            df_hist = con.execute(f"SELECT * FROM history_view WHERE code='{code}'").fetchdf()
        
        # B. è¯»å–ä»Šæ—¥
        df_new = pd.DataFrame()
        if code in kline_map:
            try: df_new = pd.read_parquet(kline_map[code])
            except: pass
            
        if df_hist.empty and df_new.empty: continue
            
        # ç»Ÿä¸€æ—¥æœŸ
        if not df_hist.empty: df_hist['date'] = pd.to_datetime(df_hist['date'])
        if not df_new.empty: df_new['date'] = pd.to_datetime(df_new['date'])
        
        # åˆå¹¶
        df = pd.concat([df_hist, df_new])
        # å¿…é¡»æ¸…é™¤ç©ºæ—¥æœŸçš„è¡Œ
        df.dropna(subset=['date'], inplace=True)
        df.drop_duplicates(subset=['code', 'date'], keep='last', inplace=True)
        df.sort_values('date', inplace=True)
        
        # D. å…³è”èµ„é‡‘æµ
        if code in flow_map:
            if os.path.exists(flow_map[code]):
                try:
                    df_flow = pd.read_parquet(flow_map[code])
                    df_flow['date'] = pd.to_datetime(df_flow['date'])
                    
                    # ä»…å¯¹ new data å…³è”ï¼Œè¿˜æ˜¯å…¨é‡ï¼Ÿå…¨é‡å…³è”æœ€ç¨³
                    df = pd.merge(df, df_flow, on=['date', 'code'], how='left', suffixes=('', '_new'))
                    
                    # åˆå¹¶æ–°æ—§èµ„é‡‘æµåˆ—
                    flow_raw_cols = ['net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow']
                    for col in flow_raw_cols:
                        if f"{col}_new" in df.columns:
                            df[col] = df[f"{col}_new"].combine_first(df[col])
                            df.drop(columns=[f"{col}_new"], inplace=True)
                except: pass

        # E. è®¡ç®—æŒ‡æ ‡
        df = calculate_indicators(df)
        
        # F. ç”Ÿæˆå‘¨/æœˆçº¿ (ä½¿ç”¨å†…å­˜df)
        process_resample(None, None, df)
        
        # å†…åµŒ Buffer æ”¶é›† (å‘¨/æœˆçº¿)
        agg_rules = {
            'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
            'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
            'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
        }
        flow_raw_cols = ['net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow']
        for c in flow_raw_cols: 
            if c in df.columns: agg_rules[c] = 'sum'
            
        try:
            w_df = df.set_index('date').resample('W-FRI').agg(agg_rules).dropna(subset=['close']).reset_index()
            w_df['code'] = code
            weekly_buffer.append(w_df)
        except: pass
        
        try:
            m_df = df.set_index('date').resample('ME').agg(agg_rules).dropna(subset=['close']).reset_index()
            m_df['code'] = code
            monthly_buffer.append(m_df)
        except: pass

        # G. å†™å…¥ Parquet
        # è¡¥é½ç¼ºå¤±åˆ— (ç”¨ NaN)
        for col in final_schema.names:
            if col not in df.columns:
                df[col] = np.nan
        
        # æŒ‰ç…§ Schema é¡ºåºé‡æ’
        df = df[final_schema.names]
        
        # å¼ºåˆ¶ç±»å‹è½¬æ¢ (Float32)
        df[float_cols_def] = df[float_cols_def].astype('float32')
        # æ—¥æœŸè½¬ String
        df['date'] = df['date'].dt.strftime('%Y-%m-%d')
        
        # å†™å…¥ Buffer
        table = pa.Table.from_pandas(df, schema=final_schema)
        writer_buffer.write_table(table)
        
        # å†™å…¥ OSS (Current Year)
        df_curr = df[df['date'] >= f"{current_year}-01-01"]
        if not df_curr.empty:
            table_curr = pa.Table.from_pandas(df_curr, schema=final_schema)
            writer_oss.write_table(table_curr)
            
        del df, table

    writer_buffer.close()
    writer_oss.close()
    print("âœ… æ—¥çº¿å†™å…¥å®Œæˆ")

    # 5. ä¿å­˜å‘¨/æœˆçº¿
    print("ğŸ“… ä¿å­˜å‘¨/æœˆçº¿...")
    if weekly_buffer:
        df_w = pd.concat(weekly_buffer, ignore_index=True)
        df_w = df_w.groupby('code', group_keys=False).apply(lambda x: calculate_indicators(x.sort_values('date')))
        
        float_cols = df_w.select_dtypes(include=['float64']).columns
        df_w[float_cols] = df_w[float_cols].astype('float32')
        df_w['date'] = df_w['date'].dt.strftime('%Y-%m-%d')
        
        df_w.to_parquet(f"{OUTPUT_ENGINE}/stock_weekly.parquet", index=False, compression='zstd')
        
    if monthly_buffer:
        df_m = pd.concat(monthly_buffer, ignore_index=True)
        df_m = df_m.groupby('code', group_keys=False).apply(lambda x: calculate_indicators(x.sort_values('date')))
        
        float_cols = df_m.select_dtypes(include=['float64']).columns
        df_m[float_cols] = df_m[float_cols].astype('float32')
        df_m['date'] = df_m['date'].dt.strftime('%Y-%m-%d')
        
        df_m.to_parquet(f"{OUTPUT_ENGINE}/stock_monthly.parquet", index=False, compression='zstd')

    print("ğŸ‰ ä»»åŠ¡å…¨éƒ¨å®Œæˆ")

if __name__ == "__main__":
    main()
