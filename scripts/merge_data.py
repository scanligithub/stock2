# scripts/merge_data.py
import pandas as pd
import glob
import os
from tqdm import tqdm
import pandas_ta as ta
import numpy as np
import duckdb
import shutil

# è·¯å¾„é…ç½®
KLINE_DIR = "downloaded_kline" 
FLOW_DIR = "downloaded_fundflow"
OUTPUT_DIR = "final_output/engine"
TEMP_DIR = "temp_chunks"

os.makedirs(OUTPUT_DIR, exist_ok=True)
if os.path.exists(TEMP_DIR):
    shutil.rmtree(TEMP_DIR)
os.makedirs(TEMP_DIR, exist_ok=True)

BATCH_SIZE = 500

def clean_indicators(df):
    """
    å¼ºåˆ¶æ¸…æ´—æŒ‡æ ‡åˆ—ï¼Œé˜²æ­¢ 'Object' ç±»å‹æˆ–æ··å…¥ Timestamp å¯¼è‡´ Parquet å´©æºƒ
    """
    # å®šä¹‰æ‰€æœ‰åº”è¯¥æ˜¯æµ®ç‚¹æ•°çš„æŒ‡æ ‡åˆ—
    target_cols = [
        # MACD
        'dif', 'dea', 'macd', 
        # KDJ
        'k', 'd', 'j',
        # RSI
        'rsi6', 'rsi12', 'rsi24',
        # BOLL
        'boll_up', 'boll_lb',
        # Other
        'cci', 'atr',
        # Funds
        'net_flow_amount', 'main_net_flow', 'super_large_net_flow', 
        'large_net_flow', 'medium_small_net_flow',
        # Factors
        'peTTM', 'pbMRQ', 'adjustFactor', 'mkt_cap'
    ]
    
    # åŠ ä¸Šæ‰€æœ‰å‡çº¿
    for w in [5, 10, 20, 60, 120, 250]:
        target_cols.append(f'ma{w}')
    for w in [5, 10, 20, 30]:
        target_cols.append(f'vol_ma{w}')

    # æ‰§è¡Œæ¸…æ´—
    for col in target_cols:
        if col in df.columns:
            # errors='coerce' æ˜¯å…³é”®ï¼šé‡åˆ°æ— æ³•è½¬æ¢çš„è„æ•°æ®(å¦‚Timestamp)ï¼Œç›´æ¥å˜NaN
            df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')
    
    return df

def process_single_stock(k_path, f_map):
    try:
        # 1. è¯»å– K çº¿
        df = pd.read_parquet(k_path)
        if df.empty: return None
        
        filename = os.path.basename(k_path)
        code = filename.replace('.parquet', '')
        
        # 2. åˆå¹¶èµ„é‡‘æµ
        if filename in f_map:
            f_path = f_map[filename]
            try:
                df_f = pd.read_parquet(f_path)
                if not df_f.empty:
                    df['date'] = pd.to_datetime(df['date'])
                    df_f['date'] = pd.to_datetime(df_f['date'])
                    df = pd.merge(df, df_f, on=['date', 'code'], how='left')
            except Exception:
                pass # èµ„é‡‘æµå‡ºé”™ä¸å½±å“ä¸»æµç¨‹
        
        # æ’åº
        if df['date'].dtype != 'datetime64[ns]':
            df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)

        # 3. è®¡ç®—æŒ‡æ ‡
        # A. MAs
        for w in [5, 10, 20, 60, 120, 250]:
            df[f'ma{w}'] = df['close'].rolling(w).mean()
        for w in [5, 10, 20, 30]:
            df[f'vol_ma{w}'] = df['volume'].rolling(w).mean()

        # B. MACD
        try:
            macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
            if macd is not None:
                df['dif'] = macd.iloc[:, 0]
                df['macd'] = macd.iloc[:, 1]
                df['dea'] = macd.iloc[:, 2]
        except: pass

        # C. KDJ
        try:
            kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
            if kdj is not None:
                df['k'] = kdj.iloc[:, 0]
                df['d'] = kdj.iloc[:, 1]
                df['j'] = kdj.iloc[:, 2]
        except: pass

        # D. RSI
        try:
            df['rsi6'] = df.ta.rsi(close='close', length=6)
            df['rsi12'] = df.ta.rsi(close='close', length=12)
            df['rsi24'] = df.ta.rsi(close='close', length=24)
        except: pass

        # E. BOLL
        try:
            boll = df.ta.bbands(close='close', length=20, std=2)
            if boll is not None:
                df['boll_lb'] = boll.iloc[:, 0]
                df['boll_up'] = boll.iloc[:, 2]
        except: pass
        
        # F. Other
        try:
            df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
            df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
        except: pass

        # ã€å…³é”®ã€‘åœ¨è¿”å›å‰å¼ºåˆ¶æ¸…æ´—ç±»å‹
        df = clean_indicators(df)

        return df

    except Exception as e:
        print(f"Err processing {k_path}: {e}")
        return None

def main():
    print("ğŸš€ å¼€å§‹å†…å­˜å®‰å…¨ç‰ˆåˆå¹¶æµç¨‹ (å¼ºç±»å‹æ¸…æ´—)...")
    
    k_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    f_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    print(f"å¾…å¤„ç†è‚¡ç¥¨: {len(k_files)}")
    
    f_map = {os.path.basename(f): f for f in f_files}
    
    batch_buffer = []
    chunk_index = 0
    
    for i, k_path in enumerate(tqdm(k_files, desc="Processing Batches")):
        df = process_single_stock(k_path, f_map)
        if df is not None:
            batch_buffer.append(df)
        
        # å†™å…¥ä¸´æ—¶å—
        if len(batch_buffer) >= BATCH_SIZE or (i == len(k_files) - 1 and batch_buffer):
            chunk_df = pd.concat(batch_buffer, ignore_index=True)
            
            # å†æ¬¡ä¿é™©ï¼šè¿˜åŸæ—¥æœŸæ ¼å¼
            chunk_df['date'] = chunk_df['date'].dt.strftime('%Y-%m-%d')
            
            temp_path = f"{TEMP_DIR}/chunk_{chunk_index}.parquet"
            # ä½¿ç”¨ pyarrow å¼•æ“ï¼Œç¡®ä¿å…¼å®¹æ€§
            chunk_df.to_parquet(temp_path, index=False, compression='zstd', engine='pyarrow')
            
            batch_buffer = []
            chunk_index += 1
            
    print(f"\nâœ… æ‰¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {chunk_index} ä¸ªä¸´æ—¶å—ã€‚å¼€å§‹æœ€ç»ˆåˆå¹¶...")

    final_output = f"{OUTPUT_DIR}/stock_full.parquet"
    
    try:
        con = duckdb.connect()
        print("ğŸ¦† DuckDB Merging...")
        
        query = f"""
        COPY (
            SELECT * FROM read_parquet('{TEMP_DIR}/*.parquet')
            ORDER BY code, date
        ) TO '{final_output}' (FORMAT 'PARQUET', COMPRESSION 'ZSTD', ROW_GROUP_SIZE 100000);
        """
        con.execute(query)
        con.close()
        
        print(f"âœ… æœ€ç»ˆæ–‡ä»¶ç”Ÿæˆå®Œæ¯•: {final_output}")
        shutil.rmtree(TEMP_DIR)
        
    except Exception as e:
        print(f"âŒ DuckDB Merge Failed: {e}")

if __name__ == "__main__":
    main()
