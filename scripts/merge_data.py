# scripts/merge_data.py
import duckdb
import pandas as pd
import pandas_ta as ta
import numpy as np
import os
import glob
import datetime
import pyarrow as pa
import pyarrow.parquet as pq
from tqdm import tqdm
import gc

# === è·¯å¾„é…ç½® ===
CACHE_DIR = "cache_data"          
KLINE_DIR = "downloaded_kline"    
FLOW_DIR = "downloaded_fundflow"  

# è¾“å‡ºç›®å½•
OUTPUT_ENGINE = "final_output/engine"
OUTPUT_DAILY = f"{OUTPUT_ENGINE}/stock_daily" 
CACHE_OUTPUT_FILE = f"{CACHE_DIR}/stock_buffer.parquet" 

os.makedirs(OUTPUT_DAILY, exist_ok=True)

# === DuckDB åˆå§‹åŒ– ===
con = duckdb.connect(database=':memory:')
con.execute("SET memory_limit='4GB';") 
con.execute("SET threads=2;")

def get_all_codes():
    """
    åˆ†åˆ«æ‰«æå†å²æ–‡ä»¶å’Œä»Šæ—¥å¢é‡æ–‡ä»¶ï¼Œè·å–å…¨é‡è‚¡ç¥¨ä»£ç 
    """
    codes = set()
    
    # 1. ä»å†å² Cache ä¸­è·å–ä»£ç  (å¦‚æœæœ‰)
    history_files = glob.glob(f"{CACHE_DIR}/*.parquet")
    if history_files:
        print(f"ğŸ“¦ æ‰«æå†å²æ–‡ä»¶: {len(history_files)} ä¸ª")
        # DuckDB è¯»å–åˆ—æå…¶å¿«
        try:
            # è¿™é‡Œçš„ files_sql æ˜¯ä¸ºäº†è®© duckdb è¯»å–åˆ—è¡¨
            files_sql = str(history_files).replace('[', '').replace(']', '')
            df_codes = con.execute(f"SELECT DISTINCT code FROM read_parquet({files_sql})").fetchdf()
            codes.update(df_codes['code'].tolist())
        except Exception as e:
            print(f"âš ï¸ è¯»å–å†å²ä»£ç å¤±è´¥ (å¯èƒ½æ˜¯ç©ºæ–‡ä»¶): {e}")

    # 2. ä»ä»Šæ—¥å¢é‡è·å–ä»£ç 
    kline_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    print(f"ğŸ”¥ æ‰«æä»Šæ—¥å¢é‡: {len(kline_files)} ä¸ª")
    for f in kline_files:
        # æ–‡ä»¶åå³ä»£ç  (ä¾‹å¦‚ sz.000001.parquet)
        code = os.path.basename(f).replace('.parquet', '')
        codes.add(code)
        
    return sorted(list(codes)), history_files, kline_files

def calculate_indicators(df):
    """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡"""
    # 1. ä»·æ ¼å‡çº¿
    for w in [5, 10, 20, 60, 120, 250]:
        df[f'ma{w}'] = df['close'].rolling(window=w).mean()
    
    # 2. æˆäº¤é‡å‡çº¿
    for w in [5, 10, 20, 30]:
        df[f'vol_ma{w}'] = df['volume'].rolling(window=w).mean()

    # 3. MACD
    try:
        macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
        if macd is not None:
            df['dif'] = macd.iloc[:, 0]
            df['macd'] = macd.iloc[:, 1]
            df['dea'] = macd.iloc[:, 2]
    except: pass

    # 4. KDJ
    try:
        kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
        if kdj is not None:
            df['k'] = kdj.iloc[:, 0]
            df['d'] = kdj.iloc[:, 1]
            df['j'] = kdj.iloc[:, 2]
    except: pass

    # 5. RSI
    try:
        df['rsi6'] = df.ta.rsi(close='close', length=6)
        df['rsi12'] = df.ta.rsi(close='close', length=12)
        df['rsi24'] = df.ta.rsi(close='close', length=24)
    except: pass

    # 6. BOLL
    try:
        boll = df.ta.bbands(close='close', length=20, std=2)
        if boll is not None:
            df['boll_lb'] = boll.iloc[:, 0]
            df['boll_up'] = boll.iloc[:, 2]
    except: pass

    # 7. å…¶ä»–
    try:
        df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
        df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
    except: pass

    return df

def process_resample(writer_w, writer_m, df_daily):
    """æµå¼ç”Ÿæˆå‘¨/æœˆçº¿"""
    if df_daily.empty: return

    agg_rules = {
        'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
        'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
        'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
    }
    # èµ„é‡‘æµç´¯åŠ 
    flow_cols = [c for c in df_daily.columns if 'flow' in c]
    for c in flow_cols: agg_rules[c] = 'sum'

    # å‘¨çº¿
    try:
        df_w = df_daily.set_index('date').resample('W-FRI').agg(agg_rules).dropna(subset=['close']).reset_index()
        if not df_w.empty:
            df_w['code'] = df_daily['code'].iloc[0]
            for w in [5, 10, 20]:
                df_w[f'ma{w}'] = df_w['close'].rolling(w).mean()
            # å†™å…¥å‰è½¬ç±»å‹
            float_cols = df_w.select_dtypes(include=['float64']).columns
            df_w[float_cols] = df_w[float_cols].astype('float32')
            table_w = pa.Table.from_pandas(df_w)
            writer_w.write_table(table_w)
    except: pass

    # æœˆçº¿
    try:
        df_m = df_daily.set_index('date').resample('ME').agg(agg_rules).dropna(subset=['close']).reset_index()
        if not df_m.empty:
            df_m['code'] = df_daily['code'].iloc[0]
            for w in [5, 10, 20]:
                df_m[f'ma{w}'] = df_m['close'].rolling(w).mean()
            float_cols = df_m.select_dtypes(include=['float64']).columns
            df_m[float_cols] = df_m[float_cols].astype('float32')
            table_m = pa.Table.from_pandas(df_m)
            writer_m.write_table(table_m)
    except: pass

def main():
    print("ğŸš€ å¼€å§‹ DuckDB æµå¼åˆå¹¶ä¸è®¡ç®— (Schema ä¿®å¤ç‰ˆ)...")
    
    # 1. è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç  & æ–‡ä»¶åˆ—è¡¨
    all_codes, history_files, kline_files = get_all_codes()
    
    if not all_codes:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è‚¡ç¥¨ä»£ç ")
        return
        
    print(f"âœ… æ€»è®¡éœ€å¤„ç†: {len(all_codes)} åªè‚¡ç¥¨")

    # å»ºç«‹æ–‡ä»¶ç´¢å¼• (Code -> Path)
    # kline_map: ä»Šæ—¥å¢é‡æ–‡ä»¶
    kline_map = {os.path.basename(f).replace('.parquet', ''): f for f in kline_files}
    # flow_map: èµ„é‡‘æµæ–‡ä»¶
    flow_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    flow_map = {os.path.basename(f).replace('.parquet', ''): f for f in flow_files}

    # 2. å¦‚æœæœ‰å†å²æ–‡ä»¶ï¼Œæ³¨å†Œåˆ° DuckDB è§†å›¾æ–¹ä¾¿æŸ¥è¯¢
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæ³¨å†Œå†å²æ–‡ä»¶ï¼Œä¸æ··å…¥ä»Šæ—¥æ–‡ä»¶ï¼Œé¿å… Schema å†²çª
    has_history = False
    if history_files:
        files_sql = str(history_files).replace('[', '').replace(']', '')
        con.execute(f"CREATE OR REPLACE VIEW history_view AS SELECT * FROM read_parquet({files_sql})")
        has_history = True

    # 3. åˆå§‹åŒ– Writers
    # ä¸ºäº†è·å–å®Œæ•´çš„ Schema (åŒ…å«æ‰€æœ‰æŒ‡æ ‡)ï¼Œæˆ‘ä»¬å…ˆæ¨¡æ‹Ÿå¤„ç†ä¸€åªè‚¡ç¥¨
    print("ğŸ” æ¨æ–­æœ€ç»ˆ Schema...")
    sample_df = pd.DataFrame()
    
    # å°è¯•æ‰¾ä¸€ä¸ªæœ‰å†å²æ•°æ®çš„è‚¡ç¥¨æ¥æ¨æ–­ Schema
    if has_history:
        try:
            sample_code = all_codes[0]
            sample_df = con.execute(f"SELECT * FROM history_view WHERE code='{sample_code}' LIMIT 10").fetchdf()
        except: pass
    
    # å¦‚æœæ²¡å†å²ï¼Œæˆ–è€…å–å¤±è´¥ï¼Œé€ ä¸€ä¸ªç©ºçš„å¸¦åŸºç¡€åˆ—çš„ DF
    if sample_df.empty:
        sample_df = pd.DataFrame(columns=['date', 'code', 'open', 'close', 'high', 'low', 'volume', 'amount', 'turn', 'pctChg', 'peTTM', 'pbMRQ', 'adjustFactor', 'mkt_cap'])
    
    # ç¡®ä¿æ—¥æœŸæ ¼å¼
    if 'date' in sample_df.columns and not sample_df.empty:
        sample_df['date'] = pd.to_datetime(sample_df['date'])
        
    # æ¨¡æ‹Ÿè®¡ç®—ä¸€æ¬¡ä»¥è·å¾—å®Œæ•´åˆ— (åŒ…å« ma5, macd ç­‰)
    sample_df = calculate_indicators(sample_df)
    
    # è¡¥é½èµ„é‡‘æµåˆ— (å¦‚æœå†å²æ•°æ®é‡Œæ²¡èµ„é‡‘æµï¼Œè¿™é‡Œè¡¥ä¸Šï¼Œé˜²æ­¢ Schema ç¼ºå¤±)
    flow_cols = ['net_flow_amount', 'main_net_flow', 'super_large_net_flow', 'large_net_flow', 'medium_small_net_flow']
    for c in flow_cols:
        if c not in sample_df.columns: sample_df[c] = np.nan

    # ç»Ÿä¸€è½¬ float32
    float_cols = sample_df.select_dtypes(include=['float64']).columns
    sample_df[float_cols] = sample_df[float_cols].astype('float32')
    
    # è·å–æœ€ç»ˆ Schema
    final_schema = pa.Table.from_pandas(sample_df).schema
    
    # å®šä¹‰å‘¨/æœˆçº¿ Schema (ç®€åŒ–ç‰ˆï¼Œå…ˆä¸å¼ºæ ¡éªŒï¼Œç”¨ append æ¨¡å¼)
    # è¿™é‡Œæˆ‘ä»¬åªåˆå§‹åŒ–ä¸»æ–‡ä»¶çš„ Writer
    
    writer_buffer = pq.ParquetWriter(CACHE_OUTPUT_FILE, final_schema, compression='zstd')
    
    current_year = datetime.datetime.now().year
    oss_file = f"{OUTPUT_DAILY}/stock_{current_year}.parquet"
    writer_oss = pq.ParquetWriter(oss_file, final_schema, compression='zstd')
    
    # å‘¨æœˆçº¿å› ä¸ºä¸å¥½é¢„åˆ¤ Schemaï¼Œæš‚å­˜å†…å­˜ listï¼Œæœ€åä¸€æ¬¡æ€§å†™
    weekly_buffer = []
    monthly_buffer = []

    # 4. ğŸš€ å¼€å§‹å¾ªç¯å¤„ç†
    print("ğŸŒŠ å¼€å§‹æµå¼å¤„ç†...")
    
    for code in tqdm(all_codes):
        # A. è¯»å–å†å² (DuckDB)
        df_hist = pd.DataFrame()
        if has_history:
            # æ£€æŸ¥è¯¥è‚¡ç¥¨æ˜¯å¦åœ¨å†å²ä¸­
            # ä¼˜åŒ–ï¼šDuckDB æŸ¥è¯¢å¸¦ WHERE code å¾ˆå¿«
            df_hist = con.execute(f"SELECT * FROM history_view WHERE code='{code}'").fetchdf()
        
        # B. è¯»å–ä»Šæ—¥å¢é‡ (Pandas)
        df_new = pd.DataFrame()
        if code in kline_map:
            try:
                df_new = pd.read_parquet(kline_map[code])
            except: pass
            
        # C. åˆå¹¶
        # Pandas concat ä¼šè‡ªåŠ¨å¯¹é½åˆ—ï¼Œç¼ºå¤±çš„åˆ—(æ¯”å¦‚ä»Šæ—¥æ•°æ®çš„MA)ä¼šå¡« NaNï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„
        if df_hist.empty and df_new.empty:
            continue
            
        # ç»Ÿä¸€æ—¥æœŸ
        if not df_hist.empty: df_hist['date'] = pd.to_datetime(df_hist['date'])
        if not df_new.empty: df_new['date'] = pd.to_datetime(df_new['date'])
        
        df = pd.concat([df_hist, df_new])
        df.drop_duplicates(subset=['code', 'date'], keep='last', inplace=True)
        df.sort_values('date', inplace=True)
        
        # D. å…³è”èµ„é‡‘æµ (ä»…å¯¹ä»Šæ—¥æ•°æ®å…³è”ï¼Œå†å²æ•°æ®é‡Œåº”è¯¥å·²ç»æœ‰äº†)
        # å¦‚æœå†å²æ•°æ®é‡Œç¼ºèµ„é‡‘æµï¼Œè¿™é‡Œä¹Ÿä¼šè¡¥å…¨
        if code in flow_map: # èµ„é‡‘æµæ–‡ä»¶åé€šå¸¸ä¹Ÿæ˜¯ code
            # æ³¨æ„ï¼šèµ„é‡‘æµæ–‡ä»¶å¯èƒ½åŒ…å«å¤šå¤©ï¼Œmerge æ—¶è¦æ³¨æ„
            # ä½†æˆ‘ä»¬çš„ flow_map å­˜çš„æ˜¯è·¯å¾„ï¼Œç›´æ¥è¯»
            pass # è¿™é‡Œç®€åŒ–ï¼Œèµ„é‡‘æµåœ¨ download é˜¶æ®µå·²ç»åŒ…å«äº†å—ï¼Ÿ
            # èµ„é‡‘æµæ˜¯å•ç‹¬ä¸‹è½½çš„ï¼Œéœ€è¦åœ¨ merge_data.py é‡Œ merge
            # æˆ‘ä»¬åœ¨ get_files_list é‡Œåªæ‹¿äº†è·¯å¾„
            
            # è¯»å–èµ„é‡‘æµ
            # ä¼˜åŒ–ï¼šåªè¯»å–æœ€è¿‘çš„èµ„é‡‘æµï¼Œé¿å…å…¨é‡è¯»
            # è¿™é‡Œç®€å•å¤„ç†ï¼šè¯»å–è¯¥è‚¡ç¥¨æ‰€æœ‰çš„èµ„é‡‘æµæ–‡ä»¶
            # æˆ‘ä»¬çš„ flow_map æ˜¯ code -> file path (å‡è®¾ flow ä¹Ÿæ˜¯æŒ‰ code åˆ†ç‰‡çš„)
            # å¦‚æœä¸‹è½½è„šæœ¬äº§ç”Ÿçš„ flow æ˜¯æŒ‰ code åˆ†ç‰‡çš„ï¼Œé‚£å°±å¯¹äº†
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„èµ„é‡‘æµæ–‡ä»¶ (downloaded_fundflow/sz.000001.parquet)
            flow_path = os.path.join(FLOW_DIR, f"{code}.parquet")
            if os.path.exists(flow_path):
                try:
                    df_flow = pd.read_parquet(flow_path)
                    df_flow['date'] = pd.to_datetime(df_flow['date'])
                    
                    # Merge (Left Join)
                    # update: ä»…å¯¹é‚£äº›è¿˜æ²¡æœ‰èµ„é‡‘æµæ•°æ®çš„è¡Œè¿›è¡Œ merge
                    # ä¸ºç®€å•èµ·è§ï¼Œç›´æ¥ mergeï¼Œpandas ä¼šå¤„ç†åç¼€ï¼Œæˆ‘ä»¬å– _y (new) è¦†ç›– _x (old) æˆ–è€… combine_first
                    # æœ€ç®€å•ï¼šç›´æ¥ mergeï¼Œå¦‚æœæœ‰é‡ååˆ—ï¼Œå–èµ„é‡‘æµè¡¨é‡Œçš„
                    
                    df = pd.merge(df, df_flow, on=['date', 'code'], how='left', suffixes=('', '_new'))
                    
                    # å¦‚æœæœ‰ _new åˆ—ï¼Œè¯´æ˜æœ‰æ›´æ–°ï¼Œè¦†ç›–å›å»
                    for col in flow_cols:
                        if f"{col}_new" in df.columns:
                            df[col] = df[f"{col}_new"].combine_first(df[col])
                            df.drop(columns=[f"{col}_new"], inplace=True)
                except: pass

        # E. è®¡ç®—æŒ‡æ ‡ (å¡«è¡¥ NaN)
        df = calculate_indicators(df)
        
        # F. ç”Ÿæˆå‘¨/æœˆçº¿ (ä½¿ç”¨å†…å­˜ä¸­çš„ df)
        process_resample(None, None, df) # é€»è¾‘ä¸å˜ï¼Œå­˜å…¥ buffer
        
        # å†…åµŒ buffer æ”¶é›†é€»è¾‘
        # (ä¸ºäº†ä»£ç ç®€æ´ï¼Œè¿™é‡Œå¤åˆ¶ process_resample é‡Œçš„ agg é€»è¾‘)
        agg_rules = {
            'open': 'first', 'close': 'last', 'high': 'max', 'low': 'min',
            'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
            'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last'
        }
        for c in flow_cols: 
            if c in df.columns: agg_rules[c] = 'sum'
            
        try:
            w_df = df.set_index('date').resample('W-FRI').agg(agg_rules).dropna(subset=['close']).reset_index()
            w_df['code'] = code
            weekly_buffer.append(w_df)
        except: pass
        
        try:
            m_df = df.set_index('date').resample('ME').agg(agg_rules).dropna(subset=['close']).reset_index()
            m_df['code'] = code
            monthly_buffer.append(m_df)
        except: pass

        # G. å†™å…¥ Parquet (Buffer & OSS)
        # ç±»å‹è½¬æ¢
        float_cols_curr = df.select_dtypes(include=['float64']).columns
        df[float_cols_curr] = df[float_cols_curr].astype('float32')
        df['date'] = df['date'].dt.strftime('%Y-%m-%d')
        
        # è¡¥é½ Schema (é˜²æ­¢æŸäº›è‚¡ç¥¨ç¼ºåˆ—)
        for col in final_schema.names:
            if col not in df.columns:
                df[col] = np.nan
        
        # æ’åºå¯¹é½
        df = df[final_schema.names]
        
        # å†™å…¥ Cache Buffer
        table = pa.Table.from_pandas(df, schema=final_schema)
        writer_buffer.write_table(table)
        
        # å†™å…¥ OSS (Current Year)
        df_curr = df[df['date'] >= f"{current_year}-01-01"]
        if not df_curr.empty:
            table_curr = pa.Table.from_pandas(df_curr, schema=final_schema)
            writer_oss.write_table(table_curr)
            
        # åƒåœ¾å›æ”¶
        del df, table
        # if i % 100 == 0: gc.collect()

    writer_buffer.close()
    writer_oss.close()
    print("âœ… æ—¥çº¿æ•°æ®å†™å…¥å®Œæˆ")

    # 5. ä¿å­˜å‘¨/æœˆçº¿
    print("ğŸ“… ä¿å­˜å‘¨/æœˆçº¿...")
    if weekly_buffer:
        df_w = pd.concat(weekly_buffer, ignore_index=True)
        # æ‰¹é‡è®¡ç®—å‘¨çº¿æŒ‡æ ‡
        df_w = df_w.groupby('code', group_keys=False).apply(lambda x: calculate_indicators(x.sort_values('date')))
        
        # å‹ç¼©
        float_cols = df_w.select_dtypes(include=['float64']).columns
        df_w[float_cols] = df_w[float_cols].astype('float32')
        df_w['date'] = df_w['date'].dt.strftime('%Y-%m-%d')
        
        df_w.to_parquet(f"{OUTPUT_ENGINE}/stock_weekly.parquet", index=False, compression='zstd')
        
    if monthly_buffer:
        df_m = pd.concat(monthly_buffer, ignore_index=True)
        df_m = df_m.groupby('code', group_keys=False).apply(lambda x: calculate_indicators(x.sort_values('date')))
        
        float_cols = df_m.select_dtypes(include=['float64']).columns
        df_m[float_cols] = df_m[float_cols].astype('float32')
        df_m['date'] = df_m['date'].dt.strftime('%Y-%m-%d')
        
        df_m.to_parquet(f"{OUTPUT_ENGINE}/stock_monthly.parquet", index=False, compression='zstd')

    print("ğŸ‰ ä»»åŠ¡å…¨éƒ¨å®Œæˆ")

if __name__ == "__main__":
    main()
