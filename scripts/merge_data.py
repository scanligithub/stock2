# scripts/merge_data.py
import pandas as pd
import glob
import os
from tqdm import tqdm
import pandas_ta as ta
import numpy as np
import duckdb
import shutil
import datetime

# --- é…ç½® ---
KLINE_DIR = "downloaded_kline" 
FLOW_DIR = "downloaded_fundflow"
CACHE_DIR = "cache_data" 
OUTPUT_BASE = "final_output/engine"

# è¾“å‡ºç›®å½•ç»“æ„
DIRS = {
    "daily": f"{OUTPUT_BASE}/stock_daily",
    "weekly": f"{OUTPUT_BASE}", 
    "monthly": f"{OUTPUT_BASE}"
}
for d in DIRS.values(): os.makedirs(d, exist_ok=True)

# å†å²å½’æ¡£æ–‡ä»¶å
ARCHIVE_FILENAME = "stock_history_2005_2024.parquet"
ARCHIVE_PATH = f"{CACHE_DIR}/{ARCHIVE_FILENAME}"

BATCH_SIZE = 500

def clean_indicators(df):
    """å¼ºåˆ¶æ¸…æ´—æŒ‡æ ‡åˆ—ï¼Œé˜²æ­¢ç±»å‹é”™è¯¯"""
    target_cols = [
        'dif', 'dea', 'macd', 'k', 'd', 'j', 'rsi6', 'rsi12', 'rsi24',
        'boll_up', 'boll_lb', 'cci', 'atr',
        'net_flow_amount', 'main_net_flow', 'super_large_net_flow', 
        'large_net_flow', 'medium_small_net_flow',
        'peTTM', 'pbMRQ', 'adjustFactor', 'mkt_cap',
        'vol_ma5', 'vol_ma10', 'vol_ma20', 'vol_ma30'
    ]
    for w in [5, 10, 20, 60, 120, 250]: target_cols.append(f'ma{w}')

    for col in target_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').astype('float32')
    return df

def calculate_all_indicators(df):
    """è®¡ç®—å•åªè‚¡ç¥¨çš„å…¨å¥—æŒ‡æ ‡"""
    df = df.sort_values('date').reset_index(drop=True)
    
    # 1. MAs
    for w in [5, 10, 20, 60, 120, 250]:
        df[f'ma{w}'] = df['close'].rolling(w).mean()
    # 2. Vol MAs
    for w in [5, 10, 20, 30]:
        df[f'vol_ma{w}'] = df['volume'].rolling(w).mean()

    # 3. Technical Indicators (Pandas-TA)
    try:
        macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
        if macd is not None:
            df['dif'] = macd.iloc[:, 0]; df['macd'] = macd.iloc[:, 1]; df['dea'] = macd.iloc[:, 2]
    except: pass

    try:
        kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
        if kdj is not None:
            df['k'] = kdj.iloc[:, 0]; df['d'] = kdj.iloc[:, 1]; df['j'] = kdj.iloc[:, 2]
    except: pass

    try:
        df['rsi6'] = df.ta.rsi(close='close', length=6)
        df['rsi12'] = df.ta.rsi(close='close', length=12)
        df['rsi24'] = df.ta.rsi(close='close', length=24)
    except: pass

    try:
        boll = df.ta.bbands(close='close', length=20, std=2)
        if boll is not None:
            df['boll_lb'] = boll.iloc[:, 0]; df['boll_up'] = boll.iloc[:, 2]
    except: pass
    
    try:
        df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
        df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
    except: pass

    return clean_indicators(df)

def main():
    print("ğŸš€ å¼€å§‹å…¨é‡åˆå¹¶ä¸å‘¨æœŸç”Ÿæˆ...")
    
    # 1. åŠ è½½å†å²å½’æ¡£ (Cold Data)
    df_history = pd.DataFrame()
    if os.path.exists(ARCHIVE_PATH):
        print(f"ğŸ§Š åŠ è½½å†å²å½’æ¡£: {ARCHIVE_PATH} ...")
        try:
            df_history = pd.read_parquet(ARCHIVE_PATH)
            df_history['date'] = pd.to_datetime(df_history['date'])
        except Exception as e:
            print(f"âš ï¸ å†å²å½’æ¡£åŠ è½½å¤±è´¥: {e}")

    # 2. åŠ è½½ä»Šæ—¥ä¸‹è½½ (New Data)
    print("ğŸ”¥ åŠ è½½ä»Šæ—¥å¢é‡æ•°æ®...")
    k_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    f_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    f_map = {os.path.basename(f): f for f in f_files}
    
    new_dfs = []
    for k_path in tqdm(k_files, desc="Reading New"):
        try:
            df = pd.read_parquet(k_path)
            filename = os.path.basename(k_path)
            code = filename.replace('.parquet', '')
            
            # Merge Flow
            if filename in f_map:
                try:
                    df_f = pd.read_parquet(f_map[filename])
                    if not df_f.empty:
                        df['date'] = pd.to_datetime(df['date'])
                        df_f['date'] = pd.to_datetime(df_f['date'])
                        df = pd.merge(df, df_f, on=['date', 'code'], how='left')
                except: pass
            
            new_dfs.append(df)
        except: pass
    
    if new_dfs:
        df_new = pd.concat(new_dfs, ignore_index=True)
        df_new['date'] = pd.to_datetime(df_new['date'])
    else:
        df_new = pd.DataFrame()

    # 3. æ‹¼æ¥å…¨é‡
    if df_history.empty and df_new.empty:
        print("âŒ æ— æ•°æ®")
        return

    print("ğŸ”„ åˆå¹¶å…¨é‡æ•°æ®...")
    df_total = pd.concat([df_history, df_new])
    # å»é‡ï¼šä¿ç•™æœ€æ–°çš„æ•°æ®
    df_total.drop_duplicates(subset=['code', 'date'], keep='last', inplace=True)
    df_total.sort_values(['code', 'date'], inplace=True)
    
    # 4. è®¡ç®—ä¸ªè‚¡å…¨é‡æŒ‡æ ‡
    print("ğŸ§® è®¡ç®—ä¸ªè‚¡æ—¥çº¿æŒ‡æ ‡ (MA/MACD/KDJ)...")
    # æ³¨æ„ï¼šæ•°æ®é‡å¤§æ—¶ groupby apply ä¼šæ…¢ï¼Œä½†è¿™æ˜¯æœ€ç¨³å¦¥æ–¹å¼
    df_total = df_total.groupby('code', group_keys=False).apply(calculate_all_indicators)
    
    # 5. åˆ‡åˆ†ä¸ä¿å­˜ä¸ªè‚¡æ•°æ®
    
    # A. ä¿å­˜ 2025 çƒ­æ•°æ®
    print("ğŸ’¾ ä¿å­˜ 2025 çƒ­æ•°æ®...")
    df_2025 = df_total[df_total['date'] >= '2025-01-01'].copy()
    df_2025['date'] = df_2025['date'].dt.strftime('%Y-%m-%d')
    df_2025.to_parquet(f"{DIRS['daily']}/stock_2025.parquet", index=False, compression='zstd')
    
    # B. ä¿å­˜ å†å²å½’æ¡£ (ä»…å½“æœ¬åœ°æ²¡æœ‰å½’æ¡£æ–‡ä»¶æ—¶ç”Ÿæˆï¼Œç”¨äºé¦–æ¬¡åˆå§‹åŒ–)
    if not os.path.exists(ARCHIVE_PATH):
        print(f"ğŸ’¾ ç”Ÿæˆå†å²å½’æ¡£è¡¥ä¸: {ARCHIVE_FILENAME}")
        df_hist_save = df_total[df_total['date'] < '2025-01-01'].copy()
        if not df_hist_save.empty:
            df_hist_save['date'] = df_hist_save['date'].dt.strftime('%Y-%m-%d')
            df_hist_save.to_parquet(f"{DIRS['daily']}/{ARCHIVE_FILENAME}", index=False, compression='zstd')

    # C. ç”Ÿæˆå‘¨çº¿/æœˆçº¿
    print("ğŸ“… ç”Ÿæˆå‘¨çº¿/æœˆçº¿æ•°æ®...")
    agg_dict = {
        'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last',
        'volume': 'sum', 'amount': 'sum', 'turn': 'mean',
        'peTTM': 'last', 'pbMRQ': 'last', 'mkt_cap': 'last', 'adjustFactor': 'last',
        'net_flow_amount': 'sum', 'main_net_flow': 'sum'
    }
    valid_agg = {k: v for k, v in agg_dict.items() if k in df_total.columns}
    
    # å‘¨çº¿
    df_weekly = df_total.set_index('date').groupby('code').resample('W-FRI').agg(valid_agg).reset_index()
    df_weekly.dropna(subset=['close'], inplace=True)
    df_weekly = df_weekly.groupby('code', group_keys=False).apply(calculate_all_indicators)
    df_weekly['date'] = df_weekly['date'].dt.strftime('%Y-%m-%d')
    df_weekly.to_parquet(f"{DIRS['weekly']}/stock_weekly.parquet", index=False, compression='zstd')
    
    # æœˆçº¿
    df_monthly = df_total.set_index('date').groupby('code').resample('ME').agg(valid_agg).reset_index()
    df_monthly.dropna(subset=['close'], inplace=True)
    df_monthly = df_monthly.groupby('code', group_keys=False).apply(calculate_all_indicators)
    df_monthly['date'] = df_monthly['date'].dt.strftime('%Y-%m-%d')
    df_monthly.to_parquet(f"{DIRS['monthly']}/stock_monthly.parquet", index=False, compression='zstd')

    # ==========================================
    # 6. ã€æ–°å¢ã€‘æ¿å—èµ„é‡‘æµèšåˆè®¡ç®—
    # ==========================================
    print("ğŸ’° æ­£åœ¨è®¡ç®—æ¿å—èµ„é‡‘æµå‘ (åŸºäºä¸ªè‚¡èšåˆ)...")
    
    sector_kline_path = f"{OUTPUT_BASE}/sector_full.parquet"
    relation_path = f"{OUTPUT_BASE}/sector_constituents.parquet"
    
    if os.path.exists(sector_kline_path) and os.path.exists(relation_path):
        try:
            # 1. å‡†å¤‡æ•°æ®ï¼šdf_total å·²ç»åœ¨å†…å­˜ä¸­ï¼ŒåŒ…å« net_flow_amount
            # 2. è¿æ¥ DuckDB
            con = duckdb.connect()
            
            # æ³¨å†Œå†…å­˜ä¸­çš„ä¸ªè‚¡è¡¨
            con.register('stock_data', df_total[['date', 'code', 'net_flow_amount', 'main_net_flow']])
            
            # è¯»å–ç£ç›˜ä¸Šçš„æ¿å—Kçº¿å’Œå…³ç³»è¡¨
            con.execute(f"CREATE TABLE sector_kline AS SELECT * FROM read_parquet('{sector_kline_path}')")
            con.execute(f"CREATE TABLE relations AS SELECT * FROM read_parquet('{relation_path}')")
            
            # 3. æ‰§è¡Œèšåˆï¼šæ¿å—Kçº¿ Left Join (å…³ç³»è¡¨ Join ä¸ªè‚¡è¡¨ Group By æ¿å—,æ—¥æœŸ)
            print("   -> æ‰§è¡Œ DuckDB SQL èšåˆ...")
            sql = """
            WITH sector_flows AS (
                SELECT 
                    r.sector_code,
                    s.date,
                    SUM(s.net_flow_amount) as net_flow_amount,
                    SUM(s.main_net_flow) as main_net_flow
                FROM stock_data s
                JOIN relations r ON s.code = r.stock_code
                GROUP BY r.sector_code, s.date
            )
            SELECT 
                k.*,
                COALESCE(f.net_flow_amount, 0) as net_flow_amount,
                COALESCE(f.main_net_flow, 0) as main_net_flow
            FROM sector_kline k
            LEFT JOIN sector_flows f ON k.code = f.sector_code AND k.date = f.date
            ORDER BY k.code, k.date
            """
            
            df_sector_final = con.execute(sql).fetchdf()
            
            # 4. è¦†ç›–ä¿å­˜
            print(f"ğŸ’¾ æ›´æ–°æ¿å—æ–‡ä»¶ (å«èµ„é‡‘æµ): {sector_kline_path}")
            df_sector_final.to_parquet(sector_kline_path, index=False, compression='zstd')
            con.close()
            
        except Exception as e:
            print(f"âŒ æ¿å—èµ„é‡‘æµè®¡ç®—å¤±è´¥: {e}")
    else:
        print(f"âš ï¸ è·³è¿‡æ¿å—è®¡ç®— (æ–‡ä»¶ç¼ºå¤±): {sector_kline_path} æˆ– {relation_path}")

    print("âœ… æ‰€æœ‰æ•°æ®å¤„ç†å®Œæ¯•ï¼")

if __name__ == "__main__":
    main()
