# scripts/merge_data.py
import pandas as pd
import glob
import os
from tqdm import tqdm
import pandas_ta as ta
import numpy as np
import duckdb
import shutil

# è·¯å¾„é…ç½®
KLINE_DIR = "downloaded_kline" 
FLOW_DIR = "downloaded_fundflow"
OUTPUT_DIR = "final_output/engine"
TEMP_DIR = "temp_chunks"  # ä¸´æ—¶ç›®å½•ï¼Œç”¨äºå­˜æ”¾åˆ†æ‰¹å¤„ç†çš„ç»“æœ

os.makedirs(OUTPUT_DIR, exist_ok=True)
if os.path.exists(TEMP_DIR):
    shutil.rmtree(TEMP_DIR)
os.makedirs(TEMP_DIR, exist_ok=True)

# å…¨å±€é…ç½®ï¼šæ¯æ‰¹å¤„ç†å¤šå°‘åªè‚¡ç¥¨ (500åªçº¦å ç”¨ 300MB å†…å­˜ï¼Œéå¸¸å®‰å…¨)
BATCH_SIZE = 500

def process_single_stock(k_path, f_map):
    """
    è¯»å–å•åªè‚¡ç¥¨ -> åˆå¹¶èµ„é‡‘æµ -> è®¡ç®—æŒ‡æ ‡ -> è¿”å›DataFrame
    """
    try:
        # 1. è¯»å– K çº¿
        df = pd.read_parquet(k_path)
        if df.empty: return None
        
        filename = os.path.basename(k_path)
        code = filename.replace('.parquet', '')
        
        # 2. åˆå¹¶èµ„é‡‘æµ
        if filename in f_map:
            f_path = f_map[filename]
            df_f = pd.read_parquet(f_path)
            if not df_f.empty:
                # è½¬æ¢æ—¥æœŸç”¨äºåˆå¹¶
                df['date'] = pd.to_datetime(df['date'])
                df_f['date'] = pd.to_datetime(df_f['date'])
                df = pd.merge(df, df_f, on=['date', 'code'], how='left')
        
        # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº (è®¡ç®—æŒ‡æ ‡å¿…é¡»)
        if df['date'].dtype != 'datetime64[ns]':
            df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)

        # ==========================================
        # 3. è®¡ç®—æŒ‡æ ‡ (MA, MACD, KDJ, RSI, BOLL)
        # ==========================================
        # æ­¤æ—¶ df åªæœ‰ä¸€åªè‚¡ç¥¨çš„æ•°æ®ï¼Œè®¡ç®—éå¸¸å¿«ä¸”çœå†…å­˜
        
        # A. å‡çº¿ (Pandas Rolling æé€Ÿç‰ˆ)
        for w in [5, 10, 20, 60, 120, 250]:
            df[f'ma{w}'] = df['close'].rolling(w).mean()
        
        # B. å‡é‡
        for w in [5, 10, 20, 30]:
            df[f'vol_ma{w}'] = df['volume'].rolling(w).mean()

        # C. å¤æ‚æŒ‡æ ‡ (Pandas-TA)
        # MACD (12, 26, 9)
        try:
            macd = df.ta.macd(close='close', fast=12, slow=26, signal=9)
            if macd is not None:
                df['dif'] = macd.iloc[:, 0]
                df['macd'] = macd.iloc[:, 1]
                df['dea'] = macd.iloc[:, 2]
        except: pass

        # KDJ (9, 3, 3)
        try:
            kdj = df.ta.kdj(high='high', low='low', close='close', length=9, signal=3)
            if kdj is not None:
                df['k'] = kdj.iloc[:, 0]
                df['d'] = kdj.iloc[:, 1]
                df['j'] = kdj.iloc[:, 2]
        except: pass

        # RSI
        try:
            df['rsi6'] = df.ta.rsi(close='close', length=6)
            df['rsi12'] = df.ta.rsi(close='close', length=12)
            df['rsi24'] = df.ta.rsi(close='close', length=24)
        except: pass

        # BOLL (20, 2)
        try:
            boll = df.ta.bbands(close='close', length=20, std=2)
            if boll is not None:
                df['boll_lb'] = boll.iloc[:, 0]
                df['boll_up'] = boll.iloc[:, 2]
        except: pass
        
        # CCI & ATR
        try:
            df['cci'] = df.ta.cci(high='high', low='low', close='close', length=14)
            df['atr'] = df.ta.atr(high='high', low='low', close='close', length=14)
        except: pass

        return df

    except Exception as e:
        print(f"Err {k_path}: {e}")
        return None

def main():
    print("ğŸš€ å¼€å§‹å†…å­˜å®‰å…¨ç‰ˆåˆå¹¶æµç¨‹...")
    
    k_files = glob.glob(f"{KLINE_DIR}/**/*.parquet", recursive=True)
    f_files = glob.glob(f"{FLOW_DIR}/**/*.parquet", recursive=True)
    print(f"å¾…å¤„ç†è‚¡ç¥¨: {len(k_files)}")
    
    f_map = {os.path.basename(f): f for f in f_files}
    
    # --- ç¬¬ä¸€é˜¶æ®µï¼šåˆ†æ‰¹å¤„ç†å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ ---
    batch_buffer = []
    chunk_index = 0
    
    for i, k_path in enumerate(tqdm(k_files, desc="Processing Batches")):
        df = process_single_stock(k_path, f_map)
        if df is not None:
            batch_buffer.append(df)
        
        # å½“ç§¯æ”’åˆ°ä¸€å®šæ•°é‡ï¼Œæˆ–è€…å¤„ç†åˆ°æœ€åä¸€ä¸ªæ–‡ä»¶æ—¶ï¼Œå†™å…¥ç£ç›˜
        if len(batch_buffer) >= BATCH_SIZE or (i == len(k_files) - 1 and batch_buffer):
            # åˆå¹¶å½“å‰æ‰¹æ¬¡
            chunk_df = pd.concat(batch_buffer, ignore_index=True)
            
            # ç±»å‹ä¼˜åŒ– (Float64 -> Float32)
            float_cols = chunk_df.select_dtypes(include=['float64']).columns
            for col in float_cols:
                chunk_df[col] = chunk_df[col].round(3).astype('float32')
            
            # è¿˜åŸæ—¥æœŸæ ¼å¼ä¸ºå­—ç¬¦ä¸²
            chunk_df['date'] = chunk_df['date'].dt.strftime('%Y-%m-%d')
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_path = f"{TEMP_DIR}/chunk_{chunk_index}.parquet"
            chunk_df.to_parquet(temp_path, index=False, compression='zstd')
            
            # æ¸…ç©ºå†…å­˜
            batch_buffer = []
            chunk_index += 1
            
    print(f"\nâœ… æ‰¹å¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {chunk_index} ä¸ªä¸´æ—¶å—ã€‚å¼€å§‹æœ€ç»ˆåˆå¹¶...")

    # --- ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ DuckDB è¿›è¡Œé›¶å†…å­˜åˆå¹¶ ---
    final_output = f"{OUTPUT_DIR}/stock_full.parquet"
    
    try:
        con = duckdb.connect()
        # DuckDB çš„é­”æ³•ï¼šç›´æ¥è¯»å–æ‰€æœ‰ chunk å¹¶æ’åºå†™å…¥ï¼Œä¸éœ€è¦æŠŠæ•°æ®åŠ è½½åˆ° Python å†…å­˜
        print("ğŸ¦† DuckDB Merging...")
        
        query = f"""
        COPY (
            SELECT * FROM read_parquet('{TEMP_DIR}/*.parquet')
            ORDER BY code, date
        ) TO '{final_output}' (FORMAT 'PARQUET', COMPRESSION 'ZSTD', ROW_GROUP_SIZE 100000);
        """
        con.execute(query)
        con.close()
        
        print(f"âœ… æœ€ç»ˆæ–‡ä»¶ç”Ÿæˆå®Œæ¯•: {final_output}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(TEMP_DIR)
        
    except Exception as e:
        print(f"âŒ DuckDB Merge Failed: {e}")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœ DuckDB å¤±è´¥ï¼Œå°è¯•ç”¨ Pandas è¯» chunks (é£é™©è¾ƒå¤§ï¼Œé€šå¸¸ä¸ä¼šèµ°åˆ°è¿™æ­¥)

if __name__ == "__main__":
    main()
