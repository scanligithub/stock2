name: A-Share Daily ETL Pipeline

on:
  schedule:
    # 北京时间 21:00 (UTC 13:00) 自动触发
    - cron: '0 13 * * *'
  workflow_dispatch:
    # 允许手动触发

jobs:
  # ====================================================
  # Job 1: 准备任务分片
  # ====================================================
  prepare:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10" # 【关键】回归 3.10 兼容性最佳
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Generate Task Slices
        run: python scripts/prepare_tasks.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/upload-artifact@v4
        with:
          name: meta-data
          path: meta_data/

  # ====================================================
  # Job 2: 并行下载 K线
  # ====================================================
  download-kline:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        task_index: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
      max-parallel: 20
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/download-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10" # 【关键】回归 3.10
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Download KLine Data
        env:
          TASK_INDEX: ${{ matrix.task_index }}
        run: python scripts/download_kline.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: kline_part_${{ matrix.task_index }}
          path: temp_kline/

  # ====================================================
  # Job 3: 并行下载 资金流向
  # ====================================================
  download-fundflow:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        task_index: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
      max-parallel: 20
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/download-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10" # 【关键】回归 3.10
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Download Fund Flow Data
        env:
          TASK_INDEX: ${{ matrix.task_index }}
        run: python scripts/download_fundflow.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: flow_part_${{ matrix.task_index }}
          path: temp_fundflow/

  # ====================================================
  # Job 4: 下载板块数据
  # ====================================================
  process-sector:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10" # 【关键】回归 3.10
          cache: 'pip'
          
      - name: Install dependencies
        run: pip install -r requirements.txt
        
      - name: Download Sector Data
        env:
          CF_WORKER_URL: ${{ secrets.CF_WORKER_URL }}
        run: python scripts/download_sector.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: sector_data
          path: final_output/engine/

  # ====================================================
  # Job 5: 合并、计算与上传
  # ====================================================
  finalize:
    needs: [download-kline, download-fundflow, process-sector, prepare]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/download-artifact@v4
        with:
          path: all_artifacts
          
      - name: Organize Artifacts
        run: |
          mkdir -p downloaded_kline downloaded_fundflow final_output/engine final_output/meta final_output/report
          find all_artifacts -name "kline_part_*" -exec cp -r {}/. downloaded_kline/ \;
          find all_artifacts -name "flow_part_*" -exec cp -r {}/. downloaded_fundflow/ \;
          cp -r all_artifacts/sector_data/* final_output/engine/ 2>/dev/null || true
          cp all_artifacts/meta-data/stock_list.json final_output/meta/

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10" # 【关键】回归 3.10
          cache: 'pip'
          
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      # 2. 执行核心 ETL
      - name: Merge Wide Table & Calc Indicators
        run: python scripts/merge_data.py

      # 3. 执行质检
      - name: Run Data Quality Check
        run: python scripts/data_quality_check.py
        
      # 4. 生成简报
      - name: Add Quality Summary
        if: success()
        run: cat final_output/report/summary.md >> $GITHUB_STEP_SUMMARY

      # 5. 上传质检 Artifact
      - uses: actions/upload-artifact@v4
        with:
          name: quality-report
          path: final_output/report/

      # 6. 备份到 Artifacts
      - name: Upload OSS Assets to Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: oss-ready-data
          path: final_output/

      # 7. 配置 OSS
      - name: Setup ossutil
        uses: manyuanrong/setup-ossutil@v2.0
        with:
          endpoint: ${{ secrets.OSS_ENDPOINT }}
          access-key-id: ${{ secrets.OSS_ACCESS_KEY_ID }}
          access-key-secret: ${{ secrets.OSS_ACCESS_KEY_SECRET }}

      # 8. 上传 OSS
      - name: Upload to OSS
        run: |
          ossutil cp -f final_output/meta/stock_list.json oss://${{ secrets.OSS_BUCKET }}/meta/stock_list.json
          ossutil cp -f final_output/engine/stock_full.parquet oss://${{ secrets.OSS_BUCKET }}/engine/stock_full.parquet --jobs=10 --part-size=10240000
          ossutil cp -f final_output/engine/sector_full.parquet oss://${{ secrets.OSS_BUCKET }}/engine/sector_full.parquet
          ossutil cp -f final_output/engine/sector_list.parquet oss://${{ secrets.OSS_BUCKET }}/engine/sector_list.parquet || true
          
          DATE_TAG=$(date +%Y%m%d)
          ossutil cp -f final_output/report/quality_report.json oss://${{ secrets.OSS_BUCKET }}/report/quality_report_${DATE_TAG}.json

          echo "✅ Pipeline Completed Successfully!"
