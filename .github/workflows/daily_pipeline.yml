name: A-Share Daily ETL Pipeline

on:
  schedule:
    # 北京时间 21:00 (UTC 13:00) 自动触发
    - cron: '0 13 * * *'
  workflow_dispatch:
    # 允许手动触发

# 赋予 GITHUB_TOKEN 写权限，用于 Cache 和 Releases 操作
permissions:
  contents: write

jobs:
  # ====================================================
  # Job 1: 准备任务分片 (Prepare)
  # ====================================================
  prepare:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10" # 锁定 3.10 以保证最佳兼容性
          cache: 'pip'
          
      - name: Install dependencies
        # 使用 --pre 允许安装 pandas_ta 的 beta 版本
        run: |
          pip install --upgrade pip
          pip install --pre -r requirements.txt
      
      - name: Generate Task Slices
        run: python scripts/prepare_tasks.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/upload-artifact@v4
        with:
          name: meta-data
          path: meta_data/

  # ====================================================
  # Job 2: 并行下载 K线 + 指标数据 (Matrix 0-19)
  # ====================================================
  download-kline:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        task_index: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
      max-parallel: 20
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/download-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
      
      - name: Download KLine Data
        env:
          TASK_INDEX: ${{ matrix.task_index }}
        run: python scripts/download_kline.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: kline_part_${{ matrix.task_index }}
          path: temp_kline/

  # ====================================================
  # Job 3: 并行下载 资金流向 (Matrix 0-19)
  # ====================================================
  download-fundflow:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        task_index: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
      max-parallel: 20
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/download-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
      
      - name: Download Fund Flow Data
        env:
          TASK_INDEX: ${{ matrix.task_index }}
        run: python scripts/download_fundflow.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: flow_part_${{ matrix.task_index }}
          path: temp_fundflow/

  # ====================================================
  # Job 4: 下载/更新 板块历史数据 (Cloudflare Proxy)
  # ====================================================
  process-sector:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
          
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
        
      - name: Download Sector Data
        env:
          CF_WORKER_URL: ${{ secrets.CF_WORKER_URL }}
        run: python scripts/download_sector.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: sector_data
          path: final_output/engine/

  # ====================================================
  # Job 5: 合并、计算、归档与上传 (Finalize)
  # ====================================================
  finalize:
    needs: [download-kline, download-fundflow, process-sector, prepare]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      # 1. 下载所有中间产物
      - uses: actions/download-artifact@v4
        with:
          path: all_artifacts
          
      - name: Organize Artifacts Structure
        run: |
          mkdir -p downloaded_kline downloaded_fundflow cache_data final_output/engine final_output/meta final_output/report
          
          # 移动 K线分片
          find all_artifacts -name "kline_part_*" -exec cp -r {}/. downloaded_kline/ \;
          
          # 移动 资金流分片
          find all_artifacts -name "flow_part_*" -exec cp -r {}/. downloaded_fundflow/ \;
          
          # 移动 板块和元数据 (使用 true 防止空目录报错)
          cp -r all_artifacts/sector_data/* final_output/engine/ 2>/dev/null || true
          cp all_artifacts/meta-data/stock_list.json final_output/meta/ 2>/dev/null || true

      # 2. 【缓存恢复】恢复当年的热数据 (Stock Hot Data)
      # 用于 merge_data.py 的逻辑 (虽然全量模式下暂未深度使用，但为增量模式预留)
      - name: Restore Current Year Cache
        uses: actions/cache/restore@v4
        with:
          path: cache_data/stock_current_year.parquet
          key: stock-hot-data-${{ github.run_id }}
          restore-keys: |
            stock-hot-data-

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
          
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
      
      # 3. 执行核心 ETL
      # 包含：合并数据 -> 计算全套指标(MA/MACD/KDJ...) -> 冷热数据切分
      - name: Merge Wide Table & Calc Indicators
        run: python scripts/merge_data.py

      # 4. 【缓存保存】保存最新的当年热数据
      - name: Save Current Year Cache
        uses: actions/cache/save@v4
        with:
          path: cache_data/stock_current_year.parquet
          key: stock-hot-data-${{ github.run_id }}

      # 5. 执行数据质量检查
      - name: Run Data Quality Check
        run: python scripts/data_quality_check.py
        
      # 6. 生成简报到 Job Summary
      - name: Add Quality Summary to Job
        if: success()
        run: cat final_output/report/summary.md >> $GITHUB_STEP_SUMMARY

      # 7. 上传质检报告 Artifact
      - uses: actions/upload-artifact@v4
        with:
          name: quality-report
          path: final_output/report/

      # 8. 【备份】上传最终结果到 GitHub Artifacts (人工检查用)
      - name: Upload OSS Assets to Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: oss-ready-data
          path: final_output/

      # 9. 配置 OSS 工具
      - name: Setup ossutil
        uses: manyuanrong/setup-ossutil@v2.0
        with:
          endpoint: ${{ secrets.OSS_ENDPOINT }}
          access-key-id: ${{ secrets.OSS_ACCESS_KEY_ID }}
          access-key-secret: ${{ secrets.OSS_ACCESS_KEY_SECRET }}

      # 10. 上传最终结果到 Aliyun OSS
      - name: Upload to OSS
        run: |
          echo "Uploading Metadata..."
          ossutil cp -f final_output/meta/stock_list.json oss://${{ secrets.OSS_BUCKET }}/meta/stock_list.json
          
          echo "Uploading Stock Data (Recursive)..."
          # 【关键】递归上传 stock_daily 目录，这将包含：
          # 1. stock_history_2005_2024.parquet (冷数据)
          # 2. stock_2025.parquet (热数据)
          ossutil cp -r -f final_output/engine/stock_daily/ oss://${{ secrets.OSS_BUCKET }}/engine/stock_daily/
          
          echo "Uploading Sector Data..."
          ossutil cp -f final_output/engine/sector_full.parquet oss://${{ secrets.OSS_BUCKET }}/engine/sector_full.parquet
          ossutil cp -f final_output/engine/sector_list.parquet oss://${{ secrets.OSS_BUCKET }}/engine/sector_list.parquet || true
          
          echo "Uploading Reports..."
          DATE_TAG=$(date +%Y%m%d)
          ossutil cp -f final_output/report/quality_report.json oss://${{ secrets.OSS_BUCKET }}/report/quality_report_${DATE_TAG}.json

          echo "✅ Pipeline Completed Successfully!"
