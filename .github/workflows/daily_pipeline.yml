name: A-Share Daily ETL Pipeline

on:
   #  schedule:
    # 北京时间 21:00 (UTC 13:00) 自动触发
   #  - cron: '0 13 * * *'
  workflow_dispatch:
    # 允许手动触发

# 赋予 GITHUB_TOKEN 写权限，用于 Releases 和 Cache 操作
permissions:
  contents: write

jobs:
  # ====================================================
  # Job 1: 准备任务分片 (Prepare)
  # ====================================================
  prepare:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install --pre -r requirements.txt
      
      - name: Generate Task Slices
        run: python scripts/prepare_tasks.py
        env:
          # 如需全量跑，保持默认或设为 false；测试设为 true
          TEST_MODE: "false"
        
      - uses: actions/upload-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/upload-artifact@v4
        with:
          name: meta-data
          path: meta_data/

  # ====================================================
  # Job 2: 并行下载 K线 (Matrix 0-19)
  # ====================================================
  download-kline:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        task_index: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
      max-parallel: 20
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/download-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
      
      - name: Download KLine Data
        env:
          TASK_INDEX: ${{ matrix.task_index }}
        run: python scripts/download_kline.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: kline_part_${{ matrix.task_index }}
          path: temp_kline/

  # ====================================================
  # Job 3: 并行下载 资金流向 (Matrix 0-19)
  # ====================================================
  download-fundflow:
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        task_index: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
      max-parallel: 20
      fail-fast: false
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/download-artifact@v4
        with:
          name: task-slices
          path: task_slices/
          
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
      
      - name: Download Fund Flow Data
        env:
          TASK_INDEX: ${{ matrix.task_index }}
        run: python scripts/download_fundflow.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: flow_part_${{ matrix.task_index }}
          path: temp_fundflow/

  # ====================================================
  # Job 4: 下载/更新 板块历史数据 (Cloudflare Proxy)
  # ====================================================
  process-sector:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
          
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
        
      - name: Download Sector Data
        env:
          CF_WORKER_URL: ${{ secrets.CF_WORKER_URL }}
        run: python scripts/download_sector.py
        
      - uses: actions/upload-artifact@v4
        with:
          name: sector_data
          path: final_output/engine/

  # ====================================================
  # Job 5: 合并、计算、归档与上传 (Finalize)
  # ====================================================
  finalize:
    needs: [download-kline, download-fundflow, process-sector, prepare]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      # 1. 下载所有中间产物
      - uses: actions/download-artifact@v4
        with:
          path: all_artifacts
          
      - name: Organize Artifacts Structure
        run: |
          mkdir -p downloaded_kline downloaded_fundflow cache_data final_output/engine final_output/meta final_output/report
          
          # 移动下载的小文件
          find all_artifacts -name "kline_part_*" -exec cp -r {}/. downloaded_kline/ \;
          find all_artifacts -name "flow_part_*" -exec cp -r {}/. downloaded_fundflow/ \;
          
          # 移动板块和元数据 (加 true 防止空目录报错)
          cp -r all_artifacts/sector_data/* final_output/engine/ 2>/dev/null || true
          cp all_artifacts/meta-data/stock_list.json final_output/meta/ 2>/dev/null || true

      # 2. 【归档策略】从 Releases 下载 2024年及以前的历史冷数据
      - name: Download Historical Archive (Cold Data)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh release download archive-init --pattern "*.parquet" --dir cache_data || echo "⚠️ No historical archive found in Releases."

      # 3. 【归档策略】从 Cache 恢复 2025年(当年)的热数据
      - name: Restore Current Year Cache (Hot Data)
        uses: actions/cache/restore@v4
        with:
          path: cache_data/stock_buffer.parquet
          # 每天生成唯一的 cache key，回退匹配最近的
          key: stock-buffer-${{ github.run_id }}
          restore-keys: |
            stock-buffer-

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: 'pip'
          
      - name: Install dependencies
        run: pip install --pre -r requirements.txt
      
      # 4. 执行核心脚本
      - name: Merge Wide Table & Calc Indicators
        run: python scripts/merge_data.py

      # 5. 【归档策略】保存 Cache (供明天增量更新用)
      - name: Save Current Year Cache
        uses: actions/cache/save@v4
        with:
          path: cache_data/stock_buffer.parquet
          key: stock-buffer-${{ github.run_id }}

      # 6. 执行质检
      - name: Run Data Quality Check
        run: python scripts/data_quality_check.py
        
      # 7. 生成简报
      - name: Add Quality Summary to Job
        if: success()
        run: cat final_output/report/summary.md >> $GITHUB_STEP_SUMMARY

      # 8. 上传质检 Artifact
      - uses: actions/upload-artifact@v4
        with:
          name: quality-report
          path: final_output/report/

      # 9. 备份 OSS Ready Data 到 Artifacts (方便下载检查)
      - name: Upload OSS Assets to Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: oss-ready-data
          path: final_output/

      # 10. 配置 OSS 工具
      - name: Setup ossutil
        uses: manyuanrong/setup-ossutil@v2.0
        with:
          endpoint: ${{ secrets.OSS_ENDPOINT }}
          access-key-id: ${{ secrets.OSS_ACCESS_KEY_ID }}
          access-key-secret: ${{ secrets.OSS_ACCESS_KEY_SECRET }}

      # 11. 上传结果到 Aliyun OSS
      - name: Upload to OSS
        run: |
          # 上传元数据
          ossutil cp -f final_output/meta/stock_list.json oss://${{ secrets.OSS_BUCKET }}/meta/stock_list.json
          
          # 上传个股宽表 (递归上传 stock_daily 目录, 里面包含 stock_20xx.parquet)
          ossutil cp -r -f final_output/engine/stock_daily/ oss://${{ secrets.OSS_BUCKET }}/engine/stock_daily/
          
          # 上传周线/月线数据
          ossutil cp -f final_output/engine/stock_weekly.parquet oss://${{ secrets.OSS_BUCKET }}/engine/stock_weekly.parquet || true
          ossutil cp -f final_output/engine/stock_monthly.parquet oss://${{ secrets.OSS_BUCKET }}/engine/stock_monthly.parquet || true
          
          # 上传板块数据
          ossutil cp -f final_output/engine/sector_full.parquet oss://${{ secrets.OSS_BUCKET }}/engine/sector_full.parquet
          ossutil cp -f final_output/engine/sector_list.parquet oss://${{ secrets.OSS_BUCKET }}/engine/sector_list.parquet || true
          
          # 归档质检报告
          DATE_TAG=$(date +%Y%m%d)
          ossutil cp -f final_output/report/quality_report.json oss://${{ secrets.OSS_BUCKET }}/report/quality_report_${DATE_TAG}.json

          echo "✅ Pipeline Completed Successfully!"
